{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Copy of imdb_market_basket_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lGRAllgArUcg",
        "z3NcLCryldUa",
        "Acn29Pgj2rY3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "protecting-prague"
      },
      "source": [
        "# Project 2: Market-basket analysis - IMDB dataset"
      ],
      "id": "protecting-prague"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tough-romance"
      },
      "source": [
        "Project for the course of Algorithms for Massive Dataset <br> Nicolas Facchinetti 961648 <br> Antonio Belotti 960822"
      ],
      "id": "tough-romance"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGRAllgArUcg"
      },
      "source": [
        "# Set up the Spark enviorment"
      ],
      "id": "lGRAllgArUcg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1-A43y-raCw"
      },
      "source": [
        "We start by dowloading and installing all the needed tool to deal with Spark. In particular we are interested in obtainig a Java enviorment since Spark in written in Scala and so it need a JVM to run. Then we can download Apache Spark 3.1.2 with Hadoop 3.2 by the Apache CDN and uncompress it. Finally we can get and install PySpark, an interface for Apache Spark in Python"
      ],
      "id": "G1-A43y-raCw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylCDuP_tsDRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5338154-7230-4b7b-8ba0-ad128733abd4"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!rm spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "id": "ylCDuP_tsDRB",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-08 07:50:13--  https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228834641 (218M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.2-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.1.2-bin-had 100%[===================>] 218.23M   192MB/s    in 1.1s    \n",
            "\n",
            "2021-09-08 07:50:15 (192 MB/s) - ‘spark-3.1.2-bin-hadoop3.2.tgz’ saved [228834641/228834641]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3amH6p0nwb7I"
      },
      "source": [
        "The next step is to correctly set the path in our remote enviorment to use the obtained tools."
      ],
      "id": "3amH6p0nwb7I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXdPB9pQvM6Y"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "id": "EXdPB9pQvM6Y",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMGP-hYawvcv"
      },
      "source": [
        "Finally we can import PySpark in the project"
      ],
      "id": "IMGP-hYawvcv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Imf5XE-w84G"
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-3.1.2-bin-hadoop3.2\")# SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "id": "2Imf5XE-w84G",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJMEDFZ34-i3"
      },
      "source": [
        "# Load preprocessed dataset from file data.zip"
      ],
      "id": "nJMEDFZ34-i3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdFAFOdx5Q6n"
      },
      "source": [
        "Use the code below do load the dataset from a preprocessed file data.zip"
      ],
      "id": "LdFAFOdx5Q6n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ0o6v56gAGE",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa1953b2-5061-43e5-c9f2-bb013542a902"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if os.path.isfile(\"data.zip\"):\n",
        "  !unzip data.zip && rm data.zip\n",
        "  data = spark.read.format(\"json\").option(\"header\", \"true\").load(\"data\").select('tconst', 'nconsts').rdd\n",
        "  data.take(5)\n",
        "else:\n",
        "  print(\"Error in loading the file.\")"
      ],
      "id": "kQ0o6v56gAGE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dd8c4b30-c428-49d9-a623-08806eb785b2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dd8c4b30-c428-49d9-a623-08806eb785b2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.zip to data.zip\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/part-00063-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00174-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00180-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00071-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00187-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00017-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00062-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00140-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00114-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00003-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00091-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00138-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00087-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00178-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00006-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00160-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00076-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00186-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00166-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00070-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00004-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00192-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00129-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00196-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00144-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00173-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00027-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00117-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00142-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00190-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00122-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00034-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00134-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00081-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00127-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00184-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00021-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00163-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00169-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00116-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00064-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00160-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00193-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00012-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00105-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00094-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00098-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00198-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00053-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00123-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00024-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00162-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00133-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00004-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00127-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00007-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00054-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00054-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00002-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00176-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00183-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00089-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00073-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00018-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00109-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00107-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00196-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00111-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00005-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00115-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00003-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/_SUCCESS           \n",
            " extracting: data/.part-00099-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00036-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00153-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00034-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00115-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00048-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00052-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00104-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00113-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00062-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00194-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00116-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00155-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00016-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00065-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00142-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00140-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00199-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00164-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00198-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00193-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00166-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00162-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00111-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00188-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00172-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00154-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00044-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00112-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00188-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00081-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00072-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00063-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00046-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00028-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00096-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00011-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00179-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00048-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00100-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00090-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00152-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00110-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00112-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00124-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00010-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00156-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00195-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00175-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00149-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00066-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00031-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00130-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00051-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00152-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00061-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00061-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00086-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00163-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00045-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00094-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00008-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00056-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00015-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00189-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00013-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00133-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00085-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00135-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00056-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00009-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00172-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00139-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00097-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00107-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00046-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00068-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00175-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00099-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00113-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00082-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00136-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00021-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00171-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00035-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00122-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00097-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00040-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00156-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00164-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00157-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00020-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00057-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00110-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00019-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00143-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00114-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00095-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00083-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00157-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00145-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00182-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00027-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00105-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00120-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00093-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00088-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00159-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00042-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00076-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00150-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00155-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00037-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00033-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00008-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00000-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00070-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00080-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00072-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00095-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00128-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00007-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00168-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00190-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00158-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00138-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00123-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00030-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/._SUCCESS.crc      \n",
            " extracting: data/.part-00108-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00096-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00185-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00014-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00089-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00143-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00059-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00102-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00100-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00186-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00147-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00117-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00184-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00146-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00191-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00102-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00055-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00151-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00084-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00036-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00043-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00039-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00170-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00025-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00167-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00029-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00174-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00015-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00025-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00146-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00104-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00083-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00148-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00139-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00161-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00125-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00005-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00165-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00012-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00101-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00173-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00001-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00041-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00009-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00085-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00137-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00047-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00038-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00031-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00060-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00101-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00026-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00176-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00020-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00108-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00199-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00029-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00075-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00118-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00171-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00016-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00165-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00023-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00022-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00182-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00177-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00032-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00119-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00066-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00181-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00141-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00197-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00185-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00043-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00120-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00065-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00103-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00148-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00147-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00154-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00019-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00069-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00041-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00187-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00086-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00000-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00079-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00169-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00028-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00132-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00030-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00055-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00197-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00145-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00052-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00064-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00057-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00118-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00180-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00098-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00177-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00124-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00134-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00135-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00093-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00001-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00073-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00136-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00051-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00106-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00011-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00121-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00091-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00079-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00045-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00018-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00023-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00074-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00090-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00077-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00050-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00080-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00074-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00059-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00149-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00060-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00129-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00037-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00002-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00039-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00181-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00042-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00158-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00022-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00109-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00014-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00087-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00151-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00167-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00121-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00038-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00119-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00125-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00150-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00126-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00084-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00049-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00159-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00144-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00106-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00131-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00077-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00130-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00131-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00153-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00103-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00168-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00026-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00049-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00053-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00006-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00033-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00128-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00170-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00010-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00078-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00013-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00032-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00071-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00067-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00040-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00067-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00069-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00024-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00068-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00183-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00047-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00161-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00192-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00141-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00179-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00050-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00058-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00035-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00078-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00082-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00088-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00044-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00092-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00189-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00191-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00126-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00092-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00137-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00075-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00017-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00178-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00132-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00058-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00194-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00195-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arabic-forwarding"
      },
      "source": [
        "# Download the dataset from Kaggle"
      ],
      "id": "arabic-forwarding"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cardiac-significance"
      },
      "source": [
        "First install the Python module of Kaggle to download the dataset from its datacenter"
      ],
      "id": "cardiac-significance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "third-confidence"
      },
      "source": [
        "!pip install kaggle"
      ],
      "id": "third-confidence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attractive-recall"
      },
      "source": [
        "Then load kaggle.json, a file containing your API credentials to be able to use the services offered by Kaggle"
      ],
      "id": "attractive-recall"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "narrow-future"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "  \n",
        "# Move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "id": "narrow-future",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "entitled-stanley"
      },
      "source": [
        "Now we can download the dataset"
      ],
      "id": "entitled-stanley"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "built-indianapolis"
      },
      "source": [
        "!kaggle datasets download 'ashirwadsangwan/imdb-dataset'"
      ],
      "id": "built-indianapolis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naked-dinner"
      },
      "source": [
        "We now must unzip the compressed archive to use it. Once done we can also remove it."
      ],
      "id": "naked-dinner"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "every-homework"
      },
      "source": [
        "!unzip imdb-dataset.zip && rm imdb-dataset.zip"
      ],
      "id": "every-homework",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2wchK4S6FSd"
      },
      "source": [
        "# Preapare the data for Spark"
      ],
      "id": "B2wchK4S6FSd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkCLCyVf6P81"
      },
      "source": [
        "We can directly load the downloaded and extracted .tsv file in a Spark DataFrame by using the command read.csv(). We directly pass to the method the columns in which we are interested."
      ],
      "id": "vkCLCyVf6P81"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9yN4DYO5rA9"
      },
      "source": [
        "df_principals = spark.read.csv(\"/content/title.principals.tsv/title.principals.tsv\", sep=r'\\t', header=True).select('tconst','nconst','category')"
      ],
      "id": "o9yN4DYO5rA9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFS4-BghiEt0"
      },
      "source": [
        "df_principals.show(10)"
      ],
      "id": "TFS4-BghiEt0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biVqVaIO6rm1"
      },
      "source": [
        "df_basics = spark.read.csv(\"/content/title.basics.tsv/title.basics.tsv\", sep=r'\\t', header=True).select('tconst','titleType')"
      ],
      "id": "biVqVaIO6rm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVXn5-qxiGjL"
      },
      "source": [
        "df_basics.show(10)"
      ],
      "id": "tVXn5-qxiGjL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYEMZq6g8V8r"
      },
      "source": [
        "By inspecting the content of the column 'category' of df_principlas we can see that there are many jobs other than actors and actress (which are the two we are interested in)"
      ],
      "id": "jYEMZq6g8V8r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ANddVhG7dOM"
      },
      "source": [
        "df_principals.select(\"category\").distinct().show()"
      ],
      "id": "_ANddVhG7dOM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chgrEDU181jc"
      },
      "source": [
        "Similarly we can do the same thing with df_basics and the column 'titleType' to see how many categories a title can have."
      ],
      "id": "chgrEDU181jc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0k9A__I8uop"
      },
      "source": [
        "df_basics.select(\"titleType\").distinct().show()"
      ],
      "id": "H0k9A__I8uop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRGTAAFK7QJ0"
      },
      "source": [
        "Once the data is loaded in a Spark DataFrame we can use the PySpark SQL module for processing the data. We start by exctracting only actors and actress from df_principals"
      ],
      "id": "WRGTAAFK7QJ0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wrOWYKr8UC3"
      },
      "source": [
        "pre = df_principals.count()\n",
        "df_principals.createOrReplaceTempView(\"PRINCIPALS\") # create a temporary table on DataFrame\n",
        "df_principals = spark.sql(\"SELECT * from PRINCIPALS WHERE category ='actor' OR category='actress'\")\n",
        "print(\"We reduced the number of row from {} to {}\".format(pre, df_principals.count()))"
      ],
      "id": "0wrOWYKr8UC3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFE8PdWZ9UAa"
      },
      "source": [
        " And then we do the same thing with movies in df_basics"
      ],
      "id": "QFE8PdWZ9UAa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7idYi-s9ZlZ"
      },
      "source": [
        "pre = df_basics.count()\n",
        "df_basics.createOrReplaceTempView(\"BASICS\") # create a temporary table on DataFrame\n",
        "df_basics = spark.sql(\"SELECT * from BASICS WHERE titleType ='movie'\")\n",
        "print(\"We reduced the number of row from {} to {}\".format(pre, df_basics.count()))"
      ],
      "id": "_7idYi-s9ZlZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhZYta_8TBU1"
      },
      "source": [
        "We can now see that we have two DataFrame, one containing only the movies and the other only the people which play as actor/actress in a title. To do the desired maket-basket analysis we have to pivot our tconst as rows, so each row stands for one titleId, and then including a list of nconst identifiers of the actors that played in it."
      ],
      "id": "AhZYta_8TBU1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg5cCJD1Swlg"
      },
      "source": [
        "df_basics.show(10)"
      ],
      "id": "Gg5cCJD1Swlg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BnEuTc0S9Xm"
      },
      "source": [
        "df_principals.show(10)"
      ],
      "id": "7BnEuTc0S9Xm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuwFSQN1VyCV"
      },
      "source": [
        "So we start by joining the two dataframe to extract from df_principals only the records with tconst related to a movie. We can also discard the category column since is no longer usefull."
      ],
      "id": "QuwFSQN1VyCV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81uUuBOVVj3a"
      },
      "source": [
        "basket_data = df_principals.join(df_basics, \"tconst\").select(df_principals.tconst, df_principals.nconst).sort(\"tconst\")"
      ],
      "id": "81uUuBOVVj3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX0u3lbuXzR-"
      },
      "source": [
        "basket_data.show(10)"
      ],
      "id": "iX0u3lbuXzR-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHywTOMEZbgu"
      },
      "source": [
        "Then we can remove hypothetical duplicated row and then aggregate the data using tconst identifier."
      ],
      "id": "xHywTOMEZbgu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYN6WQg5Vj4x"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "basket_data = basket_data.dropDuplicates()\n",
        "basket_data = basket_data.groupBy(\"tconst\").agg(F.collect_list(\"nconst\").alias(\"nconsts\")).sort('tconst')"
      ],
      "id": "IYN6WQg5Vj4x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZzSHEsFaa0G"
      },
      "source": [
        "print(\"There are {} titleId buckets\".format(basket_data.count()))\n",
        "basket_data.show(10, False)"
      ],
      "id": "IZzSHEsFaa0G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOFUnUEZclPs"
      },
      "source": [
        "As we can see above we now have the data in the correct format to do our analysis: in each row we have the identifier of a movie and in the second column the list of the idenfiers of the actors that played in it.\n",
        "Since we done all the needed pre-processing computation on the data we can transform our DataFrame in a RDD to apply map-reduce functions."
      ],
      "id": "JOFUnUEZclPs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PXtfoBH41dy"
      },
      "source": [
        "Serialize to file the RDD and download to skip the processing all the time.\n",
        "\n"
      ],
      "id": "4PXtfoBH41dy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poNWxUnLm92f"
      },
      "source": [
        "basket_data.write.format('json').save(\"data\")"
      ],
      "id": "poNWxUnLm92f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_0NCykYVZeB"
      },
      "source": [
        "!zip -r data.zip data"
      ],
      "id": "c_0NCykYVZeB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXXeT6F0Vg5q"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('data.zip')"
      ],
      "id": "MXXeT6F0Vg5q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_yuGgToVZK8"
      },
      "source": [
        "# Prove Map-reduce"
      ],
      "id": "V_yuGgToVZK8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJZitqn3Ei6F"
      },
      "source": [
        "Accediamo al campo 1 sicchè 0 è il bucket, flat perché cosi unisce tutte le row in una"
      ],
      "id": "oJZitqn3Ei6F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mnuJQJyDP81"
      },
      "source": [
        "data.flatMap(lambda row: row[1]).take(10)"
      ],
      "id": "4mnuJQJyDP81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vfem5sDEo9r"
      },
      "source": [
        "Mappiamo ogni record di autore trovato in se stesso e 1"
      ],
      "id": "2vfem5sDEo9r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PgqCjGtEoVn"
      },
      "source": [
        "data.flatMap(lambda row: (row[1]))\n",
        "  .map(lambda elem: (elem,1)).take(10)"
      ],
      "id": "1PgqCjGtEoVn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t07IEegHFRvJ"
      },
      "source": [
        "Aggiungiamo reduce che somma la parte dopo il contantore dell'attore"
      ],
      "id": "t07IEegHFRvJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMu-bK-yFTcT"
      },
      "source": [
        "data.flatMap(lambda row: (row[1])).map(lambda elem: (elem,1)).reduceByKey(lambda a,b: a+b).take(10)"
      ],
      "id": "UMu-bK-yFTcT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuPjOzl_GCGc"
      },
      "source": [
        "Aggiungiamo un threshold (almeno 200 apparizioni)"
      ],
      "id": "ZuPjOzl_GCGc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bd_UXlGHYAO"
      },
      "source": [
        "res = data.flatMap(lambda row: (row[1])) \\\n",
        "          .map(lambda elem: (elem,1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .filter(lambda x: x[1] >=200)\n",
        "res.take(10)"
      ],
      "id": "6bd_UXlGHYAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvPvtIrGMN0M"
      },
      "source": [
        "Vediamo ora per la seconda parte di apriori"
      ],
      "id": "JvPvtIrGMN0M"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-he96ytnMQVj"
      },
      "source": [
        "data.take(10)"
      ],
      "id": "-he96ytnMQVj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UGC-sTeP-R7"
      },
      "source": [
        "Prendiamo il primo record per provare e estriamo i due elementi. Scriviamo una funzione che controlla se gli elementi di una copia sono nella riga"
      ],
      "id": "7UGC-sTeP-R7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm22mtPROAXc"
      },
      "source": [
        "coppia = ['nm0063086', 'nm0183823']    #primi due attori del primo record\n",
        "\n",
        "def row_contains_elements(row, elements):\n",
        "  return all(x in row for x in elements)\n",
        "\n",
        "data.map(lambda x:x[1]).filter(lambda x: row_contains_elements(x,coppia)).take(5)\n"
      ],
      "id": "jm22mtPROAXc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVckVsR_xDWP"
      },
      "source": [
        "Proviamo ora a cercare di far generare le copie possibili ad ogni singola riga. trick per evitare doppioni. flatmap direttamente almeno sono gia spacchettate"
      ],
      "id": "dVckVsR_xDWP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u96RReA1hh9"
      },
      "source": [
        "data.take(1)"
      ],
      "id": "8u96RReA1hh9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk-9ihyWoxh_"
      },
      "source": [
        "def generate_candidate(x):\n",
        "  candidates = []\n",
        "  for a, elemA in enumerate(x):\n",
        "    for b, elemB in enumerate(x):\n",
        "      if a < b:\n",
        "        candidates.append((elemA, elemB))\n",
        "  return candidates\n",
        "\n",
        "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)).take(10)"
      ],
      "id": "Gk-9ihyWoxh_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTp40IkixO4x"
      },
      "source": [
        "Aggiungiamo poi un controllo che la copia generata sia in quelle di interesse"
      ],
      "id": "PTp40IkixO4x"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w75-jNucwKwQ"
      },
      "source": [
        "copia = [('nm0063086', 'nm0183823'), ('nm0846894', 'nm3002376')]\n",
        "\n",
        "def generate_candidate(x):\n",
        "  candidates = []\n",
        "  for a, elemA in enumerate(x):\n",
        "    for b, elemB in enumerate(x):\n",
        "      if a < b:\n",
        "        candidates.append((elemA, elemB))\n",
        "  return candidates\n",
        "\n",
        "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)).filter(lambda x: x in copia).take(3)"
      ],
      "id": "w75-jNucwKwQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxuUOII3xV_w"
      },
      "source": [
        "Vero proprio passo di map. Le tuple per qualche motivo sono hashabili"
      ],
      "id": "yxuUOII3xV_w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK5xVYz1xU2W"
      },
      "source": [
        "copia = [('nm0063086', 'nm0183823'), ('nm0846894', 'nm3002376')]\n",
        "\n",
        "def generate_candidate(x):\n",
        "  candidates = []\n",
        "  for a, elemA in enumerate(x):\n",
        "    for b, elemB in enumerate(x):\n",
        "      if a < b:\n",
        "        candidates.append((elemA, elemB))\n",
        "  return candidates\n",
        "\n",
        "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)) \\\n",
        "    .filter(lambda x: x in copia).map(lambda x: (x,1)).take(3)"
      ],
      "id": "cK5xVYz1xU2W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ykb2KwxVGu"
      },
      "source": [
        "Aggiungiamo reduce e il controllo del threshold"
      ],
      "id": "N-ykb2KwxVGu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOWrjKjYxCAG"
      },
      "source": [
        "copia = [('nm0063086', 'nm0183823'), ('nm0846894', 'nm3002376')]\n",
        "\n",
        "def generate_candidate(x):\n",
        "  candidates = []\n",
        "  for a, elemA in enumerate(x):\n",
        "    for b, elemB in enumerate(x):\n",
        "      if a < b:\n",
        "        candidates.append((elemA, elemB))\n",
        "  return candidates\n",
        "\n",
        "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)) \\\n",
        "    .filter(lambda x: x in copia) \\\n",
        "    .map(lambda x: (x,1)) \\\n",
        "    .reduceByKey(lambda a,b: a+b) \\\n",
        "    .filter(lambda x: x[1] >=1) \\\n",
        "    .take(3)\n",
        "          "
      ],
      "id": "nOWrjKjYxCAG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GsAUCCbg8ON"
      },
      "source": [
        ""
      ],
      "id": "_GsAUCCbg8ON",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti7KSSed4h34"
      },
      "source": [
        "# Apriori classic"
      ],
      "id": "ti7KSSed4h34"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTGVInR_j-Fu"
      },
      "source": [
        "We start by implementing the classic Apriori algorithm. In particular we search only for tuples and not larger itemsets."
      ],
      "id": "UTGVInR_j-Fu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AghTTfDdxcAu"
      },
      "source": [
        "from itertools import tee\n",
        "\n",
        "def apriori(partitionData, support_threshold):\n",
        "  singleton_counter = []\n",
        "  lookup_index_table = {}\n",
        "  reverse_lookup_index_table = {}\n",
        "  \n",
        "  d1, d2 = tee(partitionData, 2)\n",
        "\n",
        "  # count singletons\n",
        "  for bucket in d1:\n",
        "    for item in bucket[1]:\n",
        "      if item not in lookup_index_table:\n",
        "        # The newly discovered element is appended on the tail of the array counter\n",
        "        lookup_index_table[item] = len(singleton_counter)\n",
        "        reverse_lookup_index_table[len(singleton_counter)] = item\n",
        "        singleton_counter.append(0)\n",
        "      idx = lookup_index_table[item]\n",
        "      singleton_counter[idx] += 1\n",
        "\n",
        "  frequent_items_table = [index for index,count in enumerate(singleton_counter) if count >= support_threshold]\n",
        "  frequent_singleton = [(reverse_lookup_index_table[item], singleton_counter[item]) for item in frequent_items_table]\n",
        "  # count pairs\n",
        "  pair_counter = {}\n",
        "  for bucket in d2:\n",
        "      frequent_items_of_bucket = [lookup_index_table[item] for item in bucket[1] \n",
        "                        if lookup_index_table[item] in frequent_items_table]\n",
        "      \n",
        "      for x in frequent_items_of_bucket:\n",
        "          for y in frequent_items_of_bucket:\n",
        "              if x<y:\n",
        "                  pair_counter[(x,y)] = pair_counter.get((x,y), 0) + 1\n",
        "\n",
        "  # tuple(sorted(couple)) is done because may a couple be generated backward in a partition\n",
        "  frequent_couples = [(tuple(sorted((reverse_lookup_index_table[couple[0]], reverse_lookup_index_table[couple[1]]))), count) for couple ,count \n",
        "                      in pair_counter.items() if count >= support_threshold]\n",
        "  \n",
        "  return iter(frequent_singleton + frequent_couples)"
      ],
      "id": "AghTTfDdxcAu",
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7dZSxBd1hWQ"
      },
      "source": [
        "# Apriori with MAP-REDUCE"
      ],
      "id": "I7dZSxBd1hWQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYDq3kNh5aql"
      },
      "source": [
        "Follow an implementatio of the Apriori algorithm using a map-reduce approach. The logic in the implementation is a bit different than the one provided by the book, in particular in the approach of generating couples. Also in this case we stop our search to tuples of frequent itemsets."
      ],
      "id": "YYDq3kNh5aql"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z0pKUxbAgTF"
      },
      "source": [
        "def apriorihmap(data, support_threshold):\n",
        "    \"\"\" \n",
        "    data: Pyspark.rdd \n",
        "      [\n",
        "        [tconst, [nconst,]],\n",
        "      ]\n",
        "    \"\"\"\n",
        "    nconst_rdd = data.map(lambda x: x[1])\n",
        "\n",
        "    frequent_items_rdd = nconst_rdd.flatMap(lambda x: x) \\\n",
        "          .map(lambda elem: (elem,1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .filter(lambda x: x[1] >= support_threshold)\n",
        "\n",
        "    #print(f\"found {frequent_items_rdd.count()} frequent singletons\")\n",
        "    frequent_singletons_bv = spark.sparkContext.broadcast({k[0]:True for k in frequent_items_rdd.collect()})\n",
        "\n",
        "    def generate_candidate(x):\n",
        "      candidates = []\n",
        "      for a, elemA in enumerate(x):\n",
        "        for b, elemB in enumerate(x):\n",
        "          if a < b:\n",
        "            # the tuple may be generated backwards, sort to get rid of the problem\n",
        "            candidate_tuple = tuple(sorted((elemA, elemB)))\n",
        "            candidates.append(candidate_tuple)\n",
        "      return candidates\n",
        "    \n",
        "    frequent_couples_rdd = data.map(lambda x: x[1]) \\\n",
        "          .filter(lambda x: [elem for elem in x if frequent_singletons_bv.value.get(elem, False)])\\\n",
        "          .flatMap(lambda x: generate_candidate(x)) \\\n",
        "          .map(lambda x: (x,1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .filter(lambda x: x[1] >=support_threshold)\n",
        "\n",
        "    return frequent_items_rdd.union(frequent_couples_rdd)"
      ],
      "id": "1z0pKUxbAgTF",
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KsAA5w_21g1"
      },
      "source": [
        "# SON"
      ],
      "id": "0KsAA5w_21g1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Kk0QGUlr-X"
      },
      "source": [
        "We then decided to also implement SON to test out if there is an improvement in time complexity. In the first step of the algorithm we decided to use the classic apriori implementation done before for the map function. We decided to partion the data in a number equal to the avaiable processors in the cluster."
      ],
      "id": "P2Kk0QGUlr-X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJgYWyXy28lB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6a8b2b-cc45-4f53-86f8-9dbe1228e9c4"
      },
      "source": [
        "# empirical sweet-spot for the number of partitions (assuming every executor has 4 cores ...)\n",
        "num_partitions = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().size() * 4\n",
        "num_partitions"
      ],
      "id": "vJgYWyXy28lB",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJwtNWu9mNL3"
      },
      "source": [
        "We must define a function for the second step to properly count the number of occurrence of frequent itemsets in a partition."
      ],
      "id": "eJwtNWu9mNL3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6lKtG-gpYk"
      },
      "source": [
        "def count_in_partition(data, frequent):\n",
        "  # prepare data for processing\n",
        "  frequent = frequent.value   # extract broadcasted values\n",
        "  data = list(data)           # cast to list to iterate more than one time\n",
        "\n",
        "  # check foreach frequent itemset\n",
        "  for frequent_item in frequent:\n",
        "    # trick to cast single element to list → not remove in the str duplicate char using set()\n",
        "    if type(frequent_item) is not tuple:\n",
        "      to_check = [frequent_item]\n",
        "    else:\n",
        "      to_check = frequent_item\n",
        "      \n",
        "    c = 0     # counter\n",
        "    # and foreach row of the dataset\n",
        "    for itemset in data:\n",
        "      # check if the frequent itemset is subset of the items of the row\n",
        "      if set(to_check).issubset(itemset[1]):\n",
        "        c += 1\n",
        "    yield (frequent_item, c)"
      ],
      "id": "ff6lKtG-gpYk",
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "565n3mcLm4vX"
      },
      "source": [
        "Then the implementation of SON with a two step map-reduce. The first finds out the frequent itemsets in the partition and the latter go to count them in the dataset and filters out the ones with support greater than threshold."
      ],
      "id": "565n3mcLm4vX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImC5plrpZ4wv"
      },
      "source": [
        "def son_m_r(data, support):\n",
        "  reduced_support = support//data.getNumPartitions()\n",
        "  first_map = data.mapPartitions(lambda partition: apriori(partition, reduced_support)).map(lambda x: (x[0], 1))\n",
        "  first_reduce = first_map.reduceByKey(lambda a,b: a+b)       # possible to remove a+b ?????????????????\n",
        "\n",
        "  # extract the frequent items and broadcast them to worker nodes\n",
        "  frequent_items = [x[0] for x in first_reduce.collect()]\n",
        "  frequent_items = spark.sparkContext.broadcast(frequent_items)\n",
        "\n",
        "  second_map = data.mapPartitions(lambda partition: count_in_partition(partition, frequent_items))\n",
        "  second_reduce = second_map.reduceByKey(lambda a,b: a+b).filter(lambda x: x[1] >= support)\n",
        "  return second_reduce"
      ],
      "id": "ImC5plrpZ4wv",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3NcLCryldUa"
      },
      "source": [
        "# Demo FP Growth"
      ],
      "id": "z3NcLCryldUa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nE0iULGnbS1"
      },
      "source": [
        "To carry our experiment we decided to also use the in library implementation of FP-growth as comparison benchmark."
      ],
      "id": "1nE0iULGnbS1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz6FTwiSlgtB"
      },
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "fpGrowth = FPGrowth(itemsCol=\"nconsts\")"
      ],
      "id": "Fz6FTwiSlgtB",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka2nZvdMmjzL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "128aaa48-8d96-468b-e9bd-8d69070c126d"
      },
      "source": [
        "\"\"\"\n",
        "model = fpGrowth.fit(basket_data)\n",
        "\n",
        "# Display frequent itemsets.\n",
        "model.freqItemsets.show()\n",
        "items = model.freqItemsets\n",
        "\n",
        "# Display generated association rules.\n",
        "model.associationRules.show()\n",
        "rules = model.associationRules\n",
        "\n",
        "# transform examines the input items against all the association rules and summarize the consequents as prediction\n",
        "model.transform(basket_data).show()\n",
        "transformed = model.transform(basket_data)\n",
        "\"\"\""
      ],
      "id": "Ka2nZvdMmjzL",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmodel = fpGrowth.fit(basket_data)\\n\\n# Display frequent itemsets.\\nmodel.freqItemsets.show()\\nitems = model.freqItemsets\\n\\n# Display generated association rules.\\nmodel.associationRules.show()\\nrules = model.associationRules\\n\\n# transform examines the input items against all the association rules and summarize the consequents as prediction\\nmodel.transform(basket_data).show()\\ntransformed = model.transform(basket_data)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y3eYwlUhklx"
      },
      "source": [
        "# Test of the algorithms"
      ],
      "id": "6Y3eYwlUhklx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAmmpjQxh37m"
      },
      "source": [
        "We extract a subset of 500 rows from the dataset to test out that our algorithms work as expected. We define min_support as 1% of the count of the rows."
      ],
      "id": "MAmmpjQxh37m"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFU3nax7hwzB",
        "outputId": "d6326d62-c6bc-4b8f-c3c6-b670b6a3bbbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "minsup = 0.01\n",
        "num_rows = 500\n",
        "sup = minsup*num_rows\n",
        "\n",
        "minid = data.take(num_rows)\n",
        "minid = spark.sparkContext.parallelize(minid)\n",
        "minid.take(5)"
      ],
      "id": "DFU3nax7hwzB",
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(tconst='tt0000009', nconsts=['nm0063086', 'nm0183823', 'nm1309758']),\n",
              " Row(tconst='tt0000335', nconsts=['nm1010955', 'nm1012612', 'nm1011210', 'nm1012621', 'nm0675239', 'nm0675260']),\n",
              " Row(tconst='tt0000502', nconsts=['nm0215752', 'nm0252720']),\n",
              " Row(tconst='tt0000574', nconsts=['nm0846887', 'nm0846894', 'nm3002376', 'nm0170118']),\n",
              " Row(tconst='tt0000615', nconsts=['nm3071427', 'nm0581353', 'nm0888988', 'nm0240418', 'nm0346387', 'nm0218953'])]"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeweibBnjlt-"
      },
      "source": [
        "We start by exectuing the classic implementation of apriori. Is compulsory to before collect the data from the RDD since this is a non distributed implementation."
      ],
      "id": "UeweibBnjlt-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvT1hHJejqx-"
      },
      "source": [
        "apriori_classic = list(apriori(minid.collect(), sup))"
      ],
      "id": "bvT1hHJejqx-",
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJQ80_vgj8GV"
      },
      "source": [
        "The we have the Apriori implementation with map-reduce"
      ],
      "id": "uJQ80_vgj8GV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE7l29m1j7m9"
      },
      "source": [
        "apriori_map = apriorihmap(minid, sup).collect()"
      ],
      "id": "TE7l29m1j7m9",
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLsmu3F9k71N"
      },
      "source": [
        "Follow the implementation with SON. The data must be repartioned on the finded sweet-spot number of partitions"
      ],
      "id": "NLsmu3F9k71N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6IRRQKJ2rF7"
      },
      "source": [
        "minid = minid.repartition(num_partitions)\n",
        "son = son_m_r(minid,sup).collect()"
      ],
      "id": "y6IRRQKJ2rF7",
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dl2cFhxoIp1"
      },
      "source": [
        "Then we also train the in library implementation of FPGrowth"
      ],
      "id": "6Dl2cFhxoIp1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-JGaLC6oIL2"
      },
      "source": [
        "# initialize\n",
        "fpGrowth.setMinSupport(minsup)\n",
        "model = fpGrowth.fit(minid.toDF())\n",
        "\n",
        "# get itemsets\n",
        "fp_growth = model.freqItemsets.collect()"
      ],
      "id": "i-JGaLC6oIL2",
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP1FgwrOwz5F"
      },
      "source": [
        "Let's put the obtained result in a tabular way."
      ],
      "id": "aP1FgwrOwz5F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdmS1h6zsGCd",
        "outputId": "e2cce792-3d9f-442a-bd2e-27eec93cd5a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def order_data(data):\n",
        "  strings = []\n",
        "  tuples = []\n",
        "  for d in data:\n",
        "    if type(d[0]) == str:\n",
        "      strings.append(d)\n",
        "    else:\n",
        "      tuples.append(d)\n",
        "  strings = sorted(strings, key= lambda x:x[0])\n",
        "  tuples = sorted(tuples, key= lambda x:x[0])\n",
        "  return strings + tuples\n",
        "\n",
        "def order_fp(data):\n",
        "  strings = []\n",
        "  tuples = []\n",
        "  for d in data:\n",
        "    if len(d[0]) == 1:\n",
        "      strings.append((d[0][0], d[1]))\n",
        "    else:\n",
        "      tuples.append((tuple(sorted(d[0])),d[1]))\n",
        "  strings = sorted(strings, key= lambda x:x[0])\n",
        "  tuples = sorted(tuples, key= lambda x:x[0])\n",
        "  return strings + tuples\n",
        "\n",
        "# keep only singleton and tuples in fpgrowth result and cast itemset to tuple\n",
        "fp_growth = [item for item in fp_growth if len(item[0]) <= 2]\n",
        "\n",
        "# order the result of computations\n",
        "apriori_classic = order_data(apriori_classic)\n",
        "apriori_map = order_data(apriori_map)\n",
        "son = order_data(son)\n",
        "fp_growth = order_fp(fp_growth)\n",
        "\n",
        "df = pd.DataFrame(list(zip(apriori_classic, apriori_map, son, fp_growth)),\n",
        "               columns =['Apriori classic', 'Apriori map', 'SON', 'FPGrowth'])\n",
        "df\n"
      ],
      "id": "KdmS1h6zsGCd",
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Apriori classic</th>\n",
              "      <th>Apriori map</th>\n",
              "      <th>SON</th>\n",
              "      <th>FPGrowth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(nm0003425, 9)</td>\n",
              "      <td>(nm0003425, 9)</td>\n",
              "      <td>(nm0003425, 9)</td>\n",
              "      <td>(nm0003425, 9)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(nm0016799, 8)</td>\n",
              "      <td>(nm0016799, 8)</td>\n",
              "      <td>(nm0016799, 8)</td>\n",
              "      <td>(nm0016799, 8)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(nm0068213, 5)</td>\n",
              "      <td>(nm0068213, 5)</td>\n",
              "      <td>(nm0068213, 5)</td>\n",
              "      <td>(nm0068213, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(nm0074186, 6)</td>\n",
              "      <td>(nm0074186, 6)</td>\n",
              "      <td>(nm0074186, 6)</td>\n",
              "      <td>(nm0074186, 6)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(nm0110838, 6)</td>\n",
              "      <td>(nm0110838, 6)</td>\n",
              "      <td>(nm0110838, 6)</td>\n",
              "      <td>(nm0110838, 6)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(nm0140054, 10)</td>\n",
              "      <td>(nm0140054, 10)</td>\n",
              "      <td>(nm0140054, 10)</td>\n",
              "      <td>(nm0140054, 10)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(nm0163540, 8)</td>\n",
              "      <td>(nm0163540, 8)</td>\n",
              "      <td>(nm0163540, 8)</td>\n",
              "      <td>(nm0163540, 8)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>(nm0165691, 6)</td>\n",
              "      <td>(nm0165691, 6)</td>\n",
              "      <td>(nm0165691, 6)</td>\n",
              "      <td>(nm0165691, 6)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(nm0169878, 5)</td>\n",
              "      <td>(nm0169878, 5)</td>\n",
              "      <td>(nm0169878, 5)</td>\n",
              "      <td>(nm0169878, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(nm0190516, 7)</td>\n",
              "      <td>(nm0190516, 7)</td>\n",
              "      <td>(nm0190516, 7)</td>\n",
              "      <td>(nm0190516, 7)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>(nm0243918, 5)</td>\n",
              "      <td>(nm0243918, 5)</td>\n",
              "      <td>(nm0243918, 5)</td>\n",
              "      <td>(nm0243918, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>(nm0252476, 7)</td>\n",
              "      <td>(nm0252476, 7)</td>\n",
              "      <td>(nm0252476, 7)</td>\n",
              "      <td>(nm0252476, 7)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>(nm0292407, 10)</td>\n",
              "      <td>(nm0292407, 10)</td>\n",
              "      <td>(nm0292407, 10)</td>\n",
              "      <td>(nm0292407, 10)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>(nm0294276, 5)</td>\n",
              "      <td>(nm0294276, 5)</td>\n",
              "      <td>(nm0294276, 5)</td>\n",
              "      <td>(nm0294276, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>(nm0330280, 5)</td>\n",
              "      <td>(nm0330280, 5)</td>\n",
              "      <td>(nm0330280, 5)</td>\n",
              "      <td>(nm0330280, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>(nm0366008, 6)</td>\n",
              "      <td>(nm0366008, 6)</td>\n",
              "      <td>(nm0366008, 6)</td>\n",
              "      <td>(nm0366008, 6)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>(nm0392059, 5)</td>\n",
              "      <td>(nm0392059, 5)</td>\n",
              "      <td>(nm0392059, 5)</td>\n",
              "      <td>(nm0392059, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>(nm0505354, 7)</td>\n",
              "      <td>(nm0505354, 7)</td>\n",
              "      <td>(nm0505354, 7)</td>\n",
              "      <td>(nm0505354, 7)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>(nm0516974, 9)</td>\n",
              "      <td>(nm0516974, 9)</td>\n",
              "      <td>(nm0516974, 9)</td>\n",
              "      <td>(nm0516974, 9)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>(nm0526234, 5)</td>\n",
              "      <td>(nm0526234, 5)</td>\n",
              "      <td>(nm0526234, 5)</td>\n",
              "      <td>(nm0526234, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>(nm0528022, 7)</td>\n",
              "      <td>(nm0528022, 7)</td>\n",
              "      <td>(nm0528022, 7)</td>\n",
              "      <td>(nm0528022, 7)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>(nm0539049, 5)</td>\n",
              "      <td>(nm0539049, 5)</td>\n",
              "      <td>(nm0539049, 5)</td>\n",
              "      <td>(nm0539049, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>(nm0577476, 5)</td>\n",
              "      <td>(nm0577476, 5)</td>\n",
              "      <td>(nm0577476, 5)</td>\n",
              "      <td>(nm0577476, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>(nm0622772, 5)</td>\n",
              "      <td>(nm0622772, 5)</td>\n",
              "      <td>(nm0622772, 5)</td>\n",
              "      <td>(nm0622772, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>(nm0642190, 6)</td>\n",
              "      <td>(nm0642190, 6)</td>\n",
              "      <td>(nm0642190, 6)</td>\n",
              "      <td>(nm0642190, 6)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>(nm0676473, 7)</td>\n",
              "      <td>(nm0676473, 7)</td>\n",
              "      <td>(nm0676473, 7)</td>\n",
              "      <td>(nm0676473, 7)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>(nm0679170, 6)</td>\n",
              "      <td>(nm0679170, 6)</td>\n",
              "      <td>(nm0679170, 6)</td>\n",
              "      <td>(nm0679170, 6)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>(nm0681933, 10)</td>\n",
              "      <td>(nm0681933, 10)</td>\n",
              "      <td>(nm0681933, 10)</td>\n",
              "      <td>(nm0681933, 10)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>(nm0691995, 8)</td>\n",
              "      <td>(nm0691995, 8)</td>\n",
              "      <td>(nm0691995, 8)</td>\n",
              "      <td>(nm0691995, 8)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>(nm0746008, 6)</td>\n",
              "      <td>(nm0746008, 6)</td>\n",
              "      <td>(nm0746008, 6)</td>\n",
              "      <td>(nm0746008, 6)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>(nm0768187, 5)</td>\n",
              "      <td>(nm0768187, 5)</td>\n",
              "      <td>(nm0768187, 5)</td>\n",
              "      <td>(nm0768187, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>(nm0885818, 5)</td>\n",
              "      <td>(nm0885818, 5)</td>\n",
              "      <td>(nm0885818, 5)</td>\n",
              "      <td>(nm0885818, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>(nm0908390, 6)</td>\n",
              "      <td>(nm0908390, 6)</td>\n",
              "      <td>(nm0908390, 6)</td>\n",
              "      <td>(nm0908390, 6)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>(nm0926280, 12)</td>\n",
              "      <td>(nm0926280, 12)</td>\n",
              "      <td>(nm0926280, 12)</td>\n",
              "      <td>(nm0926280, 12)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>((nm0003425, nm0016799), 7)</td>\n",
              "      <td>((nm0003425, nm0016799), 7)</td>\n",
              "      <td>((nm0003425, nm0016799), 7)</td>\n",
              "      <td>((nm0003425, nm0016799), 7)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>((nm0140054, nm0243918), 5)</td>\n",
              "      <td>((nm0140054, nm0243918), 5)</td>\n",
              "      <td>((nm0140054, nm0243918), 5)</td>\n",
              "      <td>((nm0140054, nm0243918), 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>((nm0292407, nm0505354), 5)</td>\n",
              "      <td>((nm0292407, nm0505354), 5)</td>\n",
              "      <td>((nm0292407, nm0505354), 5)</td>\n",
              "      <td>((nm0292407, nm0505354), 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>((nm0292407, nm0642190), 5)</td>\n",
              "      <td>((nm0292407, nm0642190), 5)</td>\n",
              "      <td>((nm0292407, nm0642190), 5)</td>\n",
              "      <td>((nm0292407, nm0642190), 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>((nm0292407, nm0926280), 9)</td>\n",
              "      <td>((nm0292407, nm0926280), 9)</td>\n",
              "      <td>((nm0292407, nm0926280), 9)</td>\n",
              "      <td>((nm0292407, nm0926280), 9)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>((nm0505354, nm0926280), 5)</td>\n",
              "      <td>((nm0505354, nm0926280), 5)</td>\n",
              "      <td>((nm0505354, nm0926280), 5)</td>\n",
              "      <td>((nm0505354, nm0926280), 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>((nm0577476, nm0622772), 5)</td>\n",
              "      <td>((nm0577476, nm0622772), 5)</td>\n",
              "      <td>((nm0577476, nm0622772), 5)</td>\n",
              "      <td>((nm0577476, nm0622772), 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>((nm0642190, nm0926280), 5)</td>\n",
              "      <td>((nm0642190, nm0926280), 5)</td>\n",
              "      <td>((nm0642190, nm0926280), 5)</td>\n",
              "      <td>((nm0642190, nm0926280), 5)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Apriori classic  ...                     FPGrowth\n",
              "0                (nm0003425, 9)  ...               (nm0003425, 9)\n",
              "1                (nm0016799, 8)  ...               (nm0016799, 8)\n",
              "2                (nm0068213, 5)  ...               (nm0068213, 5)\n",
              "3                (nm0074186, 6)  ...               (nm0074186, 6)\n",
              "4                (nm0110838, 6)  ...               (nm0110838, 6)\n",
              "5               (nm0140054, 10)  ...              (nm0140054, 10)\n",
              "6                (nm0163540, 8)  ...               (nm0163540, 8)\n",
              "7                (nm0165691, 6)  ...               (nm0165691, 6)\n",
              "8                (nm0169878, 5)  ...               (nm0169878, 5)\n",
              "9                (nm0190516, 7)  ...               (nm0190516, 7)\n",
              "10               (nm0243918, 5)  ...               (nm0243918, 5)\n",
              "11               (nm0252476, 7)  ...               (nm0252476, 7)\n",
              "12              (nm0292407, 10)  ...              (nm0292407, 10)\n",
              "13               (nm0294276, 5)  ...               (nm0294276, 5)\n",
              "14               (nm0330280, 5)  ...               (nm0330280, 5)\n",
              "15               (nm0366008, 6)  ...               (nm0366008, 6)\n",
              "16               (nm0392059, 5)  ...               (nm0392059, 5)\n",
              "17               (nm0505354, 7)  ...               (nm0505354, 7)\n",
              "18               (nm0516974, 9)  ...               (nm0516974, 9)\n",
              "19               (nm0526234, 5)  ...               (nm0526234, 5)\n",
              "20               (nm0528022, 7)  ...               (nm0528022, 7)\n",
              "21               (nm0539049, 5)  ...               (nm0539049, 5)\n",
              "22               (nm0577476, 5)  ...               (nm0577476, 5)\n",
              "23               (nm0622772, 5)  ...               (nm0622772, 5)\n",
              "24               (nm0642190, 6)  ...               (nm0642190, 6)\n",
              "25               (nm0676473, 7)  ...               (nm0676473, 7)\n",
              "26               (nm0679170, 6)  ...               (nm0679170, 6)\n",
              "27              (nm0681933, 10)  ...              (nm0681933, 10)\n",
              "28               (nm0691995, 8)  ...               (nm0691995, 8)\n",
              "29               (nm0746008, 6)  ...               (nm0746008, 6)\n",
              "30               (nm0768187, 5)  ...               (nm0768187, 5)\n",
              "31               (nm0885818, 5)  ...               (nm0885818, 5)\n",
              "32               (nm0908390, 6)  ...               (nm0908390, 6)\n",
              "33              (nm0926280, 12)  ...              (nm0926280, 12)\n",
              "34  ((nm0003425, nm0016799), 7)  ...  ((nm0003425, nm0016799), 7)\n",
              "35  ((nm0140054, nm0243918), 5)  ...  ((nm0140054, nm0243918), 5)\n",
              "36  ((nm0292407, nm0505354), 5)  ...  ((nm0292407, nm0505354), 5)\n",
              "37  ((nm0292407, nm0642190), 5)  ...  ((nm0292407, nm0642190), 5)\n",
              "38  ((nm0292407, nm0926280), 9)  ...  ((nm0292407, nm0926280), 9)\n",
              "39  ((nm0505354, nm0926280), 5)  ...  ((nm0505354, nm0926280), 5)\n",
              "40  ((nm0577476, nm0622772), 5)  ...  ((nm0577476, nm0622772), 5)\n",
              "41  ((nm0642190, nm0926280), 5)  ...  ((nm0642190, nm0926280), 5)\n",
              "\n",
              "[42 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acn29Pgj2rY3"
      },
      "source": [
        "# Demo Antonio"
      ],
      "id": "Acn29Pgj2rY3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhiddpa3kiQR"
      },
      "source": [
        "import pandas as pd"
      ],
      "id": "yhiddpa3kiQR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDn6fZxXA6Fs"
      },
      "source": [
        "Lets try to load some data in a Pandas Dataframe"
      ],
      "id": "QDn6fZxXA6Fs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB6Fpgfa1El1"
      },
      "source": [
        "actors_cols = {\n",
        "    \"original\": [\n",
        "        \"nconst\",  # actor unique id\n",
        "        \"knownForTitles\"  # move he/she is in\n",
        "    ],\n",
        "    \"renamed\": [\"actorId\", \"titles\"]\n",
        "}\n",
        "\n",
        "actors_df = pd.read_csv(\n",
        "    \"name.basics.tsv.gz\",\n",
        "    compression=\"gzip\",\n",
        "    sep='\\t',\n",
        "    usecols=actors_cols[\"original\"]\n",
        ")\n",
        "\n",
        "# clean and pre-process\n",
        "actors_df.columns = actors_cols[\"renamed\"]\n",
        "actors_df.drop(actors_df[actors_df.titles == \"\\\\N\"].index, inplace=True)\n",
        "actors_df.titles = actors_df.titles.apply(lambda x: x.split(\",\"))"
      ],
      "id": "EB6Fpgfa1El1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqBeMA_JqHQS"
      },
      "source": [
        "actors_df"
      ],
      "id": "lqBeMA_JqHQS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ddwv9h4kiQR"
      },
      "source": [
        "def apriori(transactions, support_threshold):\n",
        "    singleton_counter = []\n",
        "    lookup_index_table = {}\n",
        "    reverse_lookup_index_table = {}\n",
        "\n",
        "    # count singletons\n",
        "    for bucket in transactions:\n",
        "        for elem in bucket:\n",
        "            if elem not in lookup_index_table:\n",
        "                # The newly discovered element is appended on the tail of the array counter\n",
        "                lookup_index_table[elem] = len(singleton_counter)\n",
        "                reverse_lookup_index_table[len(singleton_counter)] = elem\n",
        "                singleton_counter.append(0)\n",
        "\n",
        "            idx = lookup_index_table[elem]\n",
        "            singleton_counter[idx] += 1\n",
        "\n",
        "    frequent_items_table = [i for i,v in enumerate(singleton_counter) if v > support_threshold]\n",
        "\n",
        "    # count pairs\n",
        "    pair_counter = {}\n",
        "    for bucket in transactions:\n",
        "        frequent_items = [lookup_index_table[item] for item in bucket \n",
        "                          if lookup_index_table[item] in frequent_items_table]\n",
        "\n",
        "        for x in frequent_items:\n",
        "            for y in frequent_items:\n",
        "                if x<y:\n",
        "                    pair_counter[(x,y)] = pair_counter.get((x,y), 0) +1 \n",
        "\n",
        "    return [list(map(lambda x: reverse_lookup_index_table[x], i)) for i,c in pair_counter.items() \n",
        "            if c > support_threshold] "
      ],
      "id": "4Ddwv9h4kiQR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "x0Lw1rLXyo03",
        "outputId": "1aede69d-1e71-4f84-eed0-4a20649ef4df"
      },
      "source": [
        "apriori(data, 20)"
      ],
      "id": "x0Lw1rLXyo03",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-d6c63b5c0959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mapriori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-81-149db111dd95>\u001b[0m in \u001b[0;36mapriori\u001b[0;34m(transactions, support_threshold)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# count singletons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlookup_index_table\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'RDD' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIQAUaiDkiQS"
      },
      "source": [
        "# test\n",
        "rules = apriori(actors_df.titles, 300)\n",
        "\n",
        "movies_df = pd.read_csv(\"title.basics.tsv.gz\", compression='gzip', sep='\\t')\n",
        "from IPython.display import display\n",
        "\n",
        "for x,y in rules:\n",
        "    display(movies_df.loc[((movies_df.tconst == x) | (movies_df.tconst == y))])"
      ],
      "id": "CIQAUaiDkiQS",
      "execution_count": null,
      "outputs": []
    }
  ]
}