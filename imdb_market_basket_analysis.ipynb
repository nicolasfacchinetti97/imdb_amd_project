{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Copy of imdb_market_basket_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lGRAllgArUcg",
        "z3NcLCryldUa",
        "Acn29Pgj2rY3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "protecting-prague"
      },
      "source": [
        "# Project 2: Market-basket analysis - IMDB dataset"
      ],
      "id": "protecting-prague"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tough-romance"
      },
      "source": [
        "Project for the course of Algorithms for Massive Dataset <br> Nicolas Facchinetti 961648 <br> Antonio Belotti 960822"
      ],
      "id": "tough-romance"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGRAllgArUcg"
      },
      "source": [
        "# Set up the Spark enviorment"
      ],
      "id": "lGRAllgArUcg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1-A43y-raCw"
      },
      "source": [
        "We start by dowloading and installing all the needed tool to deal with Spark. In particular we are interested in obtainig a Java enviorment since Spark in written in Scala and so it need a JVM to run. Then we can download Apache Spark 3.1.2 with Hadoop 3.2 by the Apache CDN and uncompress it. Finally we can get and install PySpark, an interface for Apache Spark in Python"
      ],
      "id": "G1-A43y-raCw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylCDuP_tsDRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0a5782-dfa0-46bf-86e1-e2c02ef87128"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!rm spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "id": "ylCDuP_tsDRB",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-08 12:58:14--  https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228834641 (218M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.2-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.1.2-bin-had 100%[===================>] 218.23M   125MB/s    in 1.7s    \n",
            "\n",
            "2021-09-08 12:58:16 (125 MB/s) - ‘spark-3.1.2-bin-hadoop3.2.tgz’ saved [228834641/228834641]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3amH6p0nwb7I"
      },
      "source": [
        "The next step is to correctly set the path in our remote enviorment to use the obtained tools."
      ],
      "id": "3amH6p0nwb7I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXdPB9pQvM6Y"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "id": "EXdPB9pQvM6Y",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMGP-hYawvcv"
      },
      "source": [
        "Finally we can import PySpark in the project"
      ],
      "id": "IMGP-hYawvcv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Imf5XE-w84G"
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-3.1.2-bin-hadoop3.2\")# SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "id": "2Imf5XE-w84G",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJMEDFZ34-i3"
      },
      "source": [
        "# Load preprocessed dataset from file data.zip"
      ],
      "id": "nJMEDFZ34-i3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdFAFOdx5Q6n"
      },
      "source": [
        "Use the code below do load the dataset from a preprocessed file data.zip"
      ],
      "id": "LdFAFOdx5Q6n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ0o6v56gAGE",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "95a3bef0-8dc2-46d7-d943-69a204486431"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if os.path.isfile(\"data.zip\"):\n",
        "  !unzip -q data.zip && rm data.zip\n",
        "  data = spark.read.format(\"json\").option(\"header\", \"true\").load(\"data\").select('tconst', 'nconsts').rdd\n",
        "  data.take(5)\n",
        "else:\n",
        "  print(\"Error in loading the file.\")"
      ],
      "id": "kQ0o6v56gAGE",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-80a450e8-6a20-44a2-85ab-a47796112608\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-80a450e8-6a20-44a2-85ab-a47796112608\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.zip to data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arabic-forwarding"
      },
      "source": [
        "# Download the dataset from Kaggle"
      ],
      "id": "arabic-forwarding"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cardiac-significance"
      },
      "source": [
        "First install the Python module of Kaggle to download the dataset from its datacenter"
      ],
      "id": "cardiac-significance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "third-confidence"
      },
      "source": [
        "!pip install kaggle"
      ],
      "id": "third-confidence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attractive-recall"
      },
      "source": [
        "Then load kaggle.json, a file containing your API credentials to be able to use the services offered by Kaggle"
      ],
      "id": "attractive-recall"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "narrow-future"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "  \n",
        "# Move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "id": "narrow-future",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "entitled-stanley"
      },
      "source": [
        "Now we can download the dataset"
      ],
      "id": "entitled-stanley"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "built-indianapolis"
      },
      "source": [
        "!kaggle datasets download 'ashirwadsangwan/imdb-dataset'"
      ],
      "id": "built-indianapolis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naked-dinner"
      },
      "source": [
        "We now must unzip the compressed archive to use it. Once done we can also remove it."
      ],
      "id": "naked-dinner"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "every-homework"
      },
      "source": [
        "!unzip imdb-dataset.zip && rm imdb-dataset.zip"
      ],
      "id": "every-homework",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2wchK4S6FSd"
      },
      "source": [
        "# Preapare the data for Spark"
      ],
      "id": "B2wchK4S6FSd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkCLCyVf6P81"
      },
      "source": [
        "We can directly load the downloaded and extracted .tsv file in a Spark DataFrame by using the command read.csv(). We directly pass to the method the columns in which we are interested."
      ],
      "id": "vkCLCyVf6P81"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9yN4DYO5rA9"
      },
      "source": [
        "df_principals = spark.read.csv(\"/content/title.principals.tsv/title.principals.tsv\", sep=r'\\t', header=True).select('tconst','nconst','category')"
      ],
      "id": "o9yN4DYO5rA9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFS4-BghiEt0"
      },
      "source": [
        "df_principals.show(10)"
      ],
      "id": "TFS4-BghiEt0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biVqVaIO6rm1"
      },
      "source": [
        "df_basics = spark.read.csv(\"/content/title.basics.tsv/title.basics.tsv\", sep=r'\\t', header=True).select('tconst','titleType')"
      ],
      "id": "biVqVaIO6rm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVXn5-qxiGjL"
      },
      "source": [
        "df_basics.show(10)"
      ],
      "id": "tVXn5-qxiGjL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYEMZq6g8V8r"
      },
      "source": [
        "By inspecting the content of the column 'category' of df_principlas we can see that there are many jobs other than actors and actress (which are the two we are interested in)"
      ],
      "id": "jYEMZq6g8V8r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ANddVhG7dOM"
      },
      "source": [
        "df_principals.select(\"category\").distinct().show()"
      ],
      "id": "_ANddVhG7dOM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chgrEDU181jc"
      },
      "source": [
        "Similarly we can do the same thing with df_basics and the column 'titleType' to see how many categories a title can have."
      ],
      "id": "chgrEDU181jc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0k9A__I8uop"
      },
      "source": [
        "df_basics.select(\"titleType\").distinct().show()"
      ],
      "id": "H0k9A__I8uop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRGTAAFK7QJ0"
      },
      "source": [
        "Once the data is loaded in a Spark DataFrame we can use the PySpark SQL module for processing the data. We start by exctracting only actors and actress from df_principals"
      ],
      "id": "WRGTAAFK7QJ0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wrOWYKr8UC3"
      },
      "source": [
        "pre = df_principals.count()\n",
        "df_principals.createOrReplaceTempView(\"PRINCIPALS\") # create a temporary table on DataFrame\n",
        "df_principals = spark.sql(\"SELECT * from PRINCIPALS WHERE category ='actor' OR category='actress'\")\n",
        "print(\"We reduced the number of row from {} to {}\".format(pre, df_principals.count()))"
      ],
      "id": "0wrOWYKr8UC3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFE8PdWZ9UAa"
      },
      "source": [
        " And then we do the same thing with movies in df_basics"
      ],
      "id": "QFE8PdWZ9UAa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7idYi-s9ZlZ"
      },
      "source": [
        "pre = df_basics.count()\n",
        "df_basics.createOrReplaceTempView(\"BASICS\") # create a temporary table on DataFrame\n",
        "df_basics = spark.sql(\"SELECT * from BASICS WHERE titleType ='movie'\")\n",
        "print(\"We reduced the number of row from {} to {}\".format(pre, df_basics.count()))"
      ],
      "id": "_7idYi-s9ZlZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhZYta_8TBU1"
      },
      "source": [
        "We can now see that we have two DataFrame, one containing only the movies and the other only the people which play as actor/actress in a title. To do the desired maket-basket analysis we have to pivot our tconst as rows, so each row stands for one titleId, and then including a list of nconst identifiers of the actors that played in it."
      ],
      "id": "AhZYta_8TBU1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg5cCJD1Swlg"
      },
      "source": [
        "df_basics.show(10)"
      ],
      "id": "Gg5cCJD1Swlg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BnEuTc0S9Xm"
      },
      "source": [
        "df_principals.show(10)"
      ],
      "id": "7BnEuTc0S9Xm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuwFSQN1VyCV"
      },
      "source": [
        "So we start by joining the two dataframe to extract from df_principals only the records with tconst related to a movie. We can also discard the category column since is no longer usefull."
      ],
      "id": "QuwFSQN1VyCV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81uUuBOVVj3a"
      },
      "source": [
        "basket_data = df_principals.join(df_basics, \"tconst\").select(df_principals.tconst, df_principals.nconst).sort(\"tconst\")"
      ],
      "id": "81uUuBOVVj3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX0u3lbuXzR-"
      },
      "source": [
        "basket_data.show(10)"
      ],
      "id": "iX0u3lbuXzR-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHywTOMEZbgu"
      },
      "source": [
        "Then we can remove hypothetical duplicated row and then aggregate the data using tconst identifier."
      ],
      "id": "xHywTOMEZbgu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYN6WQg5Vj4x"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "basket_data = basket_data.dropDuplicates()\n",
        "basket_data = basket_data.groupBy(\"tconst\").agg(F.collect_list(\"nconst\").alias(\"nconsts\")).sort('tconst')"
      ],
      "id": "IYN6WQg5Vj4x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZzSHEsFaa0G"
      },
      "source": [
        "print(\"There are {} titleId buckets\".format(basket_data.count()))\n",
        "basket_data.show(10, False)"
      ],
      "id": "IZzSHEsFaa0G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOFUnUEZclPs"
      },
      "source": [
        "As we can see above we now have the data in the correct format to do our analysis: in each row we have the identifier of a movie and in the second column the list of the idenfiers of the actors that played in it.\n",
        "Since we done all the needed pre-processing computation on the data we can transform our DataFrame in a RDD to apply map-reduce functions."
      ],
      "id": "JOFUnUEZclPs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PXtfoBH41dy"
      },
      "source": [
        "Serialize to file the RDD and download to skip the processing all the time.\n",
        "\n"
      ],
      "id": "4PXtfoBH41dy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poNWxUnLm92f"
      },
      "source": [
        "basket_data.write.format('json').save(\"data\")"
      ],
      "id": "poNWxUnLm92f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_0NCykYVZeB"
      },
      "source": [
        "!zip -r data.zip data"
      ],
      "id": "c_0NCykYVZeB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXXeT6F0Vg5q"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('data.zip')"
      ],
      "id": "MXXeT6F0Vg5q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti7KSSed4h34"
      },
      "source": [
        "# Apriori classic"
      ],
      "id": "ti7KSSed4h34"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTGVInR_j-Fu"
      },
      "source": [
        "We start by implementing the classic Apriori algorithm. In particular we search only for tuples and not larger itemsets."
      ],
      "id": "UTGVInR_j-Fu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AghTTfDdxcAu"
      },
      "source": [
        "from itertools import tee\n",
        "\n",
        "def apriori(partitionData, support_threshold):\n",
        "  singleton_counter = []\n",
        "  lookup_index_table = {}\n",
        "  reverse_lookup_index_table = {}\n",
        "  \n",
        "  d1, d2 = tee(partitionData, 2)\n",
        "\n",
        "  # count singletons\n",
        "  for bucket in d1:\n",
        "    for item in bucket[1]:\n",
        "      if item not in lookup_index_table:\n",
        "        # The newly discovered element is appended on the tail of the array counter\n",
        "        lookup_index_table[item] = len(singleton_counter)\n",
        "        reverse_lookup_index_table[len(singleton_counter)] = item\n",
        "        singleton_counter.append(0)\n",
        "      idx = lookup_index_table[item]\n",
        "      singleton_counter[idx] += 1\n",
        "\n",
        "  frequent_items_table = [index for index,count in enumerate(singleton_counter) if count >= support_threshold]\n",
        "  frequent_singleton = [(reverse_lookup_index_table[item], singleton_counter[item]) for item in frequent_items_table]\n",
        "  # count pairs\n",
        "  pair_counter = {}\n",
        "  for bucket in d2:\n",
        "      frequent_items_of_bucket = [lookup_index_table[item] for item in bucket[1] \n",
        "                        if lookup_index_table[item] in frequent_items_table]\n",
        "      \n",
        "      for x in frequent_items_of_bucket:\n",
        "          for y in frequent_items_of_bucket:\n",
        "              if x<y:\n",
        "                  pair_counter[(x,y)] = pair_counter.get((x,y), 0) + 1\n",
        "\n",
        "  # tuple(sorted(couple)) is done because may a couple be generated backward in a partition\n",
        "  frequent_couples = [(tuple(sorted((reverse_lookup_index_table[couple[0]], reverse_lookup_index_table[couple[1]]))), count) for couple ,count \n",
        "                      in pair_counter.items() if count >= support_threshold]\n",
        "  \n",
        "  return iter(frequent_singleton + frequent_couples)"
      ],
      "id": "AghTTfDdxcAu",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7dZSxBd1hWQ"
      },
      "source": [
        "# Apriori with MAP-REDUCE"
      ],
      "id": "I7dZSxBd1hWQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYDq3kNh5aql"
      },
      "source": [
        "Follow an implementatio of the Apriori algorithm using a map-reduce approach. The logic in the implementation is a bit different than the one provided by the book, in particular in the approach of generating couples. Also in this case we stop our search to tuples of frequent itemsets."
      ],
      "id": "YYDq3kNh5aql"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z0pKUxbAgTF"
      },
      "source": [
        "def apriorihmap(data, support_threshold):\n",
        "    \"\"\" \n",
        "    data: Pyspark.rdd \n",
        "      [\n",
        "        [tconst, [nconst,]],\n",
        "      ]\n",
        "    \"\"\"\n",
        "    nconst_rdd = data.map(lambda x: x[1])\n",
        "\n",
        "    frequent_items_rdd = nconst_rdd.flatMap(lambda x: x) \\\n",
        "          .map(lambda elem: (elem,1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .filter(lambda x: x[1] >= support_threshold)\n",
        "\n",
        "    #print(f\"found {frequent_items_rdd.count()} frequent singletons\")\n",
        "    frequent_singletons_bv = spark.sparkContext.broadcast({k[0]:True for k in frequent_items_rdd.collect()})\n",
        "\n",
        "    def generate_candidate(x):\n",
        "      candidates = []\n",
        "      for a in x:\n",
        "        for b in x:\n",
        "          if a < b:\n",
        "            # the tuple may be generated backwards, sort to get rid of the problem\n",
        "            candidates.append((a,b))\n",
        "      return candidates\n",
        "    \n",
        "    frequent_couples_rdd = data.map(lambda x: x[1]) \\\n",
        "          .filter(lambda x: [elem for elem in x if frequent_singletons_bv.value.get(elem, False)])\\\n",
        "          .flatMap(lambda x: generate_candidate(x)) \\\n",
        "          .map(lambda x: (x,1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .filter(lambda x: x[1] >=support_threshold)\n",
        "\n",
        "    return frequent_items_rdd.union(frequent_couples_rdd)"
      ],
      "id": "1z0pKUxbAgTF",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KsAA5w_21g1"
      },
      "source": [
        "# SON"
      ],
      "id": "0KsAA5w_21g1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Kk0QGUlr-X"
      },
      "source": [
        "We then decided to also implement SON to test out if there is an improvement in time complexity. In the first step of the algorithm we decided to use the classic apriori implementation done before for the map function. We decided to partion the data in a number equal to the avaiable processors in the cluster."
      ],
      "id": "P2Kk0QGUlr-X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJgYWyXy28lB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f059432f-7509-4512-c46d-9fc700151262"
      },
      "source": [
        "# empirical sweet-spot for the number of partitions (assuming every executor has 4 cores ...)\n",
        "num_partitions = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().size() * 4\n",
        "num_partitions"
      ],
      "id": "vJgYWyXy28lB",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJwtNWu9mNL3"
      },
      "source": [
        "We must define a function for the second step to properly count the number of occurrence of frequent itemsets in a partition."
      ],
      "id": "eJwtNWu9mNL3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6lKtG-gpYk"
      },
      "source": [
        "def count_in_partition(data, frequent):\n",
        "  # prepare data for processing\n",
        "  frequent = frequent.value   # extract broadcasted values\n",
        "  data = list(data)           # cast to list to iterate more than one time\n",
        "\n",
        "  # check foreach frequent itemset\n",
        "  for frequent_item in frequent:\n",
        "    # trick to cast single element to list → not remove in the str duplicate char using set()\n",
        "    if type(frequent_item) is not tuple:\n",
        "      to_check = [frequent_item]\n",
        "    else:\n",
        "      to_check = frequent_item\n",
        "      \n",
        "    c = 0     # counter\n",
        "    # and foreach row of the dataset\n",
        "    for itemset in data:\n",
        "      # check if the frequent itemset is subset of the items of the row\n",
        "      if set(to_check).issubset(itemset[1]):\n",
        "        c += 1\n",
        "    yield (frequent_item, c)"
      ],
      "id": "ff6lKtG-gpYk",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "565n3mcLm4vX"
      },
      "source": [
        "Then the implementation of SON with a two step map-reduce. The first finds out the frequent itemsets in the partition and the latter go to count them in the dataset and filters out the ones with support greater than threshold."
      ],
      "id": "565n3mcLm4vX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImC5plrpZ4wv"
      },
      "source": [
        "def son_m_r(data, support):\n",
        "  reduced_support = support//data.getNumPartitions()\n",
        "  first_map = data.mapPartitions(lambda partition: apriori(partition, reduced_support)).map(lambda x: (x[0], 1))\n",
        "  first_reduce = first_map.reduceByKey(lambda a,b: a+b)       # possible to remove a+b ?????????????????\n",
        "\n",
        "  # extract the frequent items and broadcast them to worker nodes\n",
        "  frequent_items = [x[0] for x in first_reduce.collect()]\n",
        "  frequent_items = spark.sparkContext.broadcast(frequent_items)\n",
        "\n",
        "  second_map = data.mapPartitions(lambda partition: count_in_partition(partition, frequent_items))\n",
        "  second_reduce = second_map.reduceByKey(lambda a,b: a+b).filter(lambda x: x[1] >= support)\n",
        "  return second_reduce"
      ],
      "id": "ImC5plrpZ4wv",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3NcLCryldUa"
      },
      "source": [
        "# Demo FP Growth"
      ],
      "id": "z3NcLCryldUa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nE0iULGnbS1"
      },
      "source": [
        "To carry our experiment we decided to also use the in library implementation of FP-growth as comparison benchmark."
      ],
      "id": "1nE0iULGnbS1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz6FTwiSlgtB"
      },
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "fpGrowth = FPGrowth(itemsCol=\"nconsts\")"
      ],
      "id": "Fz6FTwiSlgtB",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka2nZvdMmjzL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d497288a-11cf-4861-c4a8-0beb8b6943b0"
      },
      "source": [
        "\"\"\"\n",
        "model = fpGrowth.fit(basket_data)\n",
        "\n",
        "# Display frequent itemsets.\n",
        "model.freqItemsets.show()\n",
        "items = model.freqItemsets\n",
        "\n",
        "# Display generated association rules.\n",
        "model.associationRules.show()\n",
        "rules = model.associationRules\n",
        "\n",
        "# transform examines the input items against all the association rules and summarize the consequents as prediction\n",
        "model.transform(basket_data).show()\n",
        "transformed = model.transform(basket_data)\n",
        "\"\"\""
      ],
      "id": "Ka2nZvdMmjzL",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmodel = fpGrowth.fit(basket_data)\\n\\n# Display frequent itemsets.\\nmodel.freqItemsets.show()\\nitems = model.freqItemsets\\n\\n# Display generated association rules.\\nmodel.associationRules.show()\\nrules = model.associationRules\\n\\n# transform examines the input items against all the association rules and summarize the consequents as prediction\\nmodel.transform(basket_data).show()\\ntransformed = model.transform(basket_data)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y3eYwlUhklx"
      },
      "source": [
        "# Test of the algorithms"
      ],
      "id": "6Y3eYwlUhklx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAmmpjQxh37m"
      },
      "source": [
        "We extract a subset of 500 rows from the dataset to test out that our algorithms work as expected. We define min_support as 1% of the count of the rows."
      ],
      "id": "MAmmpjQxh37m"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFU3nax7hwzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c354fcf3-f8f3-4f4a-fdfd-42f33e734e4a"
      },
      "source": [
        "minsup = 0.01\n",
        "num_rows = 500\n",
        "sup = minsup*num_rows\n",
        "\n",
        "minid = data.take(num_rows)\n",
        "minid = spark.sparkContext.parallelize(minid)\n",
        "minid.take(5)"
      ],
      "id": "DFU3nax7hwzB",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(tconst='tt0000009', nconsts=['nm0063086', 'nm0183823', 'nm1309758']),\n",
              " Row(tconst='tt0000335', nconsts=['nm1010955', 'nm1012612', 'nm1011210', 'nm1012621', 'nm0675239', 'nm0675260']),\n",
              " Row(tconst='tt0000502', nconsts=['nm0215752', 'nm0252720']),\n",
              " Row(tconst='tt0000574', nconsts=['nm0846887', 'nm0846894', 'nm3002376', 'nm0170118']),\n",
              " Row(tconst='tt0000615', nconsts=['nm3071427', 'nm0581353', 'nm0888988', 'nm0240418', 'nm0346387', 'nm0218953'])]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeweibBnjlt-"
      },
      "source": [
        "We start by exectuing the classic implementation of apriori. Is compulsory to before collect the data from the RDD since this is a non distributed implementation."
      ],
      "id": "UeweibBnjlt-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvT1hHJejqx-"
      },
      "source": [
        "apriori_classic = list(apriori(minid.collect(), sup))"
      ],
      "id": "bvT1hHJejqx-",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJQ80_vgj8GV"
      },
      "source": [
        "The we have the Apriori implementation with map-reduce"
      ],
      "id": "uJQ80_vgj8GV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE7l29m1j7m9"
      },
      "source": [
        "apriori_map = apriorihmap(minid, sup).collect()"
      ],
      "id": "TE7l29m1j7m9",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLsmu3F9k71N"
      },
      "source": [
        "Follow the implementation with SON. The data must be repartioned on the finded sweet-spot number of partitions"
      ],
      "id": "NLsmu3F9k71N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6IRRQKJ2rF7"
      },
      "source": [
        "minid = minid.repartition(num_partitions)\n",
        "son = son_m_r(minid,sup).collect()"
      ],
      "id": "y6IRRQKJ2rF7",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dl2cFhxoIp1"
      },
      "source": [
        "Then we also train the in-library implementation of FPGrowth"
      ],
      "id": "6Dl2cFhxoIp1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-JGaLC6oIL2"
      },
      "source": [
        "# initialize\n",
        "fpGrowth.setMinSupport(minsup)\n",
        "model = fpGrowth.fit(minid.toDF())\n",
        "\n",
        "# get itemsets\n",
        "fp_growth = model.freqItemsets.collect()"
      ],
      "id": "i-JGaLC6oIL2",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdmS1h6zsGCd"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def trasform_format(data):\n",
        "  strings = []\n",
        "  tuples = []\n",
        "  for d in data:\n",
        "    if len(d[0]) == 1:\n",
        "      strings.append((d[0][0], d[1]))\n",
        "    else:\n",
        "      tuples.append((tuple(sorted(d[0])),d[1]))\n",
        "  return strings + tuples\n",
        "\n",
        "# keep only singleton and tuples in fpgrowth result and trasform the format of results\n",
        "fp_growth = [item for item in fp_growth if len(item[0]) <= 2]\n",
        "fp_growth = trasform_format(fp_growth)"
      ],
      "id": "KdmS1h6zsGCd",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP1FgwrOwz5F"
      },
      "source": [
        "Let's put the obtained result in a tabular way."
      ],
      "id": "aP1FgwrOwz5F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOfyQmTHiyfM",
        "outputId": "19052518-3774-4e95-91c0-ac95dffc8b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df1 = pd.DataFrame([x[1] for x in apriori_classic], index=[x[0] for x in apriori_classic], columns =['Apriori classic'])\n",
        "df2 = pd.DataFrame([x[1] for x in apriori_map], index=[x[0] for x in apriori_map], columns =['Apriori map'])\n",
        "df3 = pd.DataFrame([x[1] for x in son], index=[x[0] for x in son], columns =['SON'])\n",
        "df4 = pd.DataFrame([x[1] for x in fp_growth], index=[x[0] for x in fp_growth], columns =['FPGrowth'])\n",
        "\n",
        "df = pd.concat([df1, df2, df3, df4], axis=1)            \n",
        "df"
      ],
      "id": "lOfyQmTHiyfM",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Apriori classic</th>\n",
              "      <th>Apriori map</th>\n",
              "      <th>SON</th>\n",
              "      <th>FPGrowth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>nm0140054</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0691995</th>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0243918</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0169878</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0539049</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0330280</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0768187</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0528022</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0681933</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0003425</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0016799</th>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0679170</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0294276</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0516974</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0252476</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0074186</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0110838</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0505354</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0292407</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0926280</th>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0642190</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0526234</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0746008</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0676473</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0392059</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0622772</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0577476</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0908390</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0163540</th>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0190516</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0068213</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0165691</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0366008</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nm0885818</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(nm0140054, nm0243918)</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(nm0003425, nm0016799)</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(nm0292407, nm0505354)</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(nm0505354, nm0926280)</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(nm0292407, nm0926280)</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(nm0292407, nm0642190)</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(nm0642190, nm0926280)</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(nm0577476, nm0622772)</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Apriori classic  Apriori map  SON  FPGrowth\n",
              "nm0140054                            10           10   10        10\n",
              "nm0691995                             8            8    8         8\n",
              "nm0243918                             5            5    5         5\n",
              "nm0169878                             5            5    5         5\n",
              "nm0539049                             5            5    5         5\n",
              "nm0330280                             5            5    5         5\n",
              "nm0768187                             5            5    5         5\n",
              "nm0528022                             7            7    7         7\n",
              "nm0681933                            10           10   10        10\n",
              "nm0003425                             9            9    9         9\n",
              "nm0016799                             8            8    8         8\n",
              "nm0679170                             6            6    6         6\n",
              "nm0294276                             5            5    5         5\n",
              "nm0516974                             9            9    9         9\n",
              "nm0252476                             7            7    7         7\n",
              "nm0074186                             6            6    6         6\n",
              "nm0110838                             6            6    6         6\n",
              "nm0505354                             7            7    7         7\n",
              "nm0292407                            10           10   10        10\n",
              "nm0926280                            12           12   12        12\n",
              "nm0642190                             6            6    6         6\n",
              "nm0526234                             5            5    5         5\n",
              "nm0746008                             6            6    6         6\n",
              "nm0676473                             7            7    7         7\n",
              "nm0392059                             5            5    5         5\n",
              "nm0622772                             5            5    5         5\n",
              "nm0577476                             5            5    5         5\n",
              "nm0908390                             6            6    6         6\n",
              "nm0163540                             8            8    8         8\n",
              "nm0190516                             7            7    7         7\n",
              "nm0068213                             5            5    5         5\n",
              "nm0165691                             6            6    6         6\n",
              "nm0366008                             6            6    6         6\n",
              "nm0885818                             5            5    5         5\n",
              "(nm0140054, nm0243918)                5            5    5         5\n",
              "(nm0003425, nm0016799)                7            7    7         7\n",
              "(nm0292407, nm0505354)                5            5    5         5\n",
              "(nm0505354, nm0926280)                5            5    5         5\n",
              "(nm0292407, nm0926280)                9            9    9         9\n",
              "(nm0292407, nm0642190)                5            5    5         5\n",
              "(nm0642190, nm0926280)                5            5    5         5\n",
              "(nm0577476, nm0622772)                5            5    5         5"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXj2acThjYp9"
      },
      "source": [
        ""
      ],
      "id": "LXj2acThjYp9",
      "execution_count": 49,
      "outputs": []
    }
  ]
}