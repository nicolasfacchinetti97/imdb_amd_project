{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of imdb_market_basket_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lGRAllgArUcg",
        "arabic-forwarding",
        "B2wchK4S6FSd",
        "I7dZSxBd1hWQ",
        "0KsAA5w_21g1",
        "z3NcLCryldUa"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "protecting-prague"
      },
      "source": [
        "# Project 2: Market-basket analysis - IMDB dataset"
      ],
      "id": "protecting-prague"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tough-romance"
      },
      "source": [
        "Project for the course of Algorithms for Massive Dataset <br> Nicolas Facchinetti 961648 <br> Antonio Belotti 960822"
      ],
      "id": "tough-romance"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGRAllgArUcg"
      },
      "source": [
        "# Set up the Spark enviorment"
      ],
      "id": "lGRAllgArUcg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1-A43y-raCw"
      },
      "source": [
        "We start by dowloading and installing all the needed tool to deal with Spark. In particular we are interested in obtainig a Java enviorment since Spark in written in Scala and so it need a JVM to run. Then we can download Apache Spark 3.1.2 with Hadoop 3.2 by the Apache CDN and uncompress it. Finally we can get and install PySpark, an interface for Apache Spark in Python"
      ],
      "id": "G1-A43y-raCw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylCDuP_tsDRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b455cee-d864-42e3-bcc2-b41f064305e5"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!rm spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "id": "ylCDuP_tsDRB",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-10 14:50:03--  https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228834641 (218M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.2-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.1.2-bin-had 100%[===================>] 218.23M   172MB/s    in 1.3s    \n",
            "\n",
            "2021-09-10 14:50:04 (172 MB/s) - ‘spark-3.1.2-bin-hadoop3.2.tgz’ saved [228834641/228834641]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3amH6p0nwb7I"
      },
      "source": [
        "The next step is to correctly set the path in our remote enviorment to use the obtained tools."
      ],
      "id": "3amH6p0nwb7I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXdPB9pQvM6Y"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "id": "EXdPB9pQvM6Y",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMGP-hYawvcv"
      },
      "source": [
        "Finally we can import PySpark in the project"
      ],
      "id": "IMGP-hYawvcv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Imf5XE-w84G"
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-3.1.2-bin-hadoop3.2\")# SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "id": "2Imf5XE-w84G",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJMEDFZ34-i3"
      },
      "source": [
        "# Load preprocessed dataset from file data.zip"
      ],
      "id": "nJMEDFZ34-i3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdFAFOdx5Q6n"
      },
      "source": [
        "Use the code below do load the dataset from a preprocessed file data.zip"
      ],
      "id": "LdFAFOdx5Q6n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ0o6v56gAGE",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "3639083b-3d23-4c28-8c3a-4740883e57dc"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if os.path.isfile(\"data.zip\"):\n",
        "  !unzip -q data.zip && rm data.zip\n",
        "  data = spark.read.format(\"json\").option(\"header\", \"true\").load(\"data\").select('tconst', 'nconsts').rdd\n",
        "  data.take(5)\n",
        "else:\n",
        "  print(\"Error in loading the file.\")"
      ],
      "id": "kQ0o6v56gAGE",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7667fcd0-f1c1-4b6d-a285-bc08b7cb990f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7667fcd0-f1c1-4b6d-a285-bc08b7cb990f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.zip to data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arabic-forwarding"
      },
      "source": [
        "# Download the dataset from Kaggle"
      ],
      "id": "arabic-forwarding"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cardiac-significance"
      },
      "source": [
        "First install the Python module of Kaggle to download the dataset from its datacenter"
      ],
      "id": "cardiac-significance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "third-confidence"
      },
      "source": [
        "!pip install kaggle"
      ],
      "id": "third-confidence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attractive-recall"
      },
      "source": [
        "Then load kaggle.json, a file containing your API credentials to be able to use the services offered by Kaggle"
      ],
      "id": "attractive-recall"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "narrow-future"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "  \n",
        "# Move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "id": "narrow-future",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "entitled-stanley"
      },
      "source": [
        "Now we can download the dataset"
      ],
      "id": "entitled-stanley"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "built-indianapolis"
      },
      "source": [
        "!kaggle datasets download 'ashirwadsangwan/imdb-dataset'"
      ],
      "id": "built-indianapolis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naked-dinner"
      },
      "source": [
        "We now must unzip the compressed archive to use it. Once done we can also remove it."
      ],
      "id": "naked-dinner"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "every-homework"
      },
      "source": [
        "!unzip imdb-dataset.zip && rm imdb-dataset.zip"
      ],
      "id": "every-homework",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2wchK4S6FSd"
      },
      "source": [
        "# Preapare the data for Spark"
      ],
      "id": "B2wchK4S6FSd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkCLCyVf6P81"
      },
      "source": [
        "We can directly load the downloaded and extracted .tsv file in a Spark DataFrame by using the command read.csv(). We directly pass to the method the columns in which we are interested."
      ],
      "id": "vkCLCyVf6P81"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9yN4DYO5rA9"
      },
      "source": [
        "df_principals = spark.read.csv(\"/content/title.principals.tsv/title.principals.tsv\", sep=r'\\t', header=True).select('tconst','nconst','category')"
      ],
      "id": "o9yN4DYO5rA9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFS4-BghiEt0"
      },
      "source": [
        "df_principals.show(10)"
      ],
      "id": "TFS4-BghiEt0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biVqVaIO6rm1"
      },
      "source": [
        "df_basics = spark.read.csv(\"/content/title.basics.tsv/title.basics.tsv\", sep=r'\\t', header=True).select('tconst','titleType')"
      ],
      "id": "biVqVaIO6rm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVXn5-qxiGjL"
      },
      "source": [
        "df_basics.show(10)"
      ],
      "id": "tVXn5-qxiGjL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYEMZq6g8V8r"
      },
      "source": [
        "By inspecting the content of the column 'category' of df_principlas we can see that there are many jobs other than actors and actress (which are the two we are interested in)"
      ],
      "id": "jYEMZq6g8V8r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ANddVhG7dOM"
      },
      "source": [
        "df_principals.select(\"category\").distinct().show()"
      ],
      "id": "_ANddVhG7dOM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chgrEDU181jc"
      },
      "source": [
        "Similarly we can do the same thing with df_basics and the column 'titleType' to see how many categories a title can have."
      ],
      "id": "chgrEDU181jc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0k9A__I8uop"
      },
      "source": [
        "df_basics.select(\"titleType\").distinct().show()"
      ],
      "id": "H0k9A__I8uop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRGTAAFK7QJ0"
      },
      "source": [
        "Once the data is loaded in a Spark DataFrame we can use the PySpark SQL module for processing the data. We start by exctracting only actors and actress from df_principals"
      ],
      "id": "WRGTAAFK7QJ0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wrOWYKr8UC3"
      },
      "source": [
        "pre = df_principals.count()\n",
        "df_principals.createOrReplaceTempView(\"PRINCIPALS\") # create a temporary table on DataFrame\n",
        "df_principals = spark.sql(\"SELECT * from PRINCIPALS WHERE category ='actor' OR category='actress'\")\n",
        "print(\"We reduced the number of row from {} to {}\".format(pre, df_principals.count()))"
      ],
      "id": "0wrOWYKr8UC3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFE8PdWZ9UAa"
      },
      "source": [
        " And then we do the same thing with movies in df_basics"
      ],
      "id": "QFE8PdWZ9UAa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7idYi-s9ZlZ"
      },
      "source": [
        "pre = df_basics.count()\n",
        "df_basics.createOrReplaceTempView(\"BASICS\") # create a temporary table on DataFrame\n",
        "df_basics = spark.sql(\"SELECT * from BASICS WHERE titleType ='movie'\")\n",
        "print(\"We reduced the number of row from {} to {}\".format(pre, df_basics.count()))"
      ],
      "id": "_7idYi-s9ZlZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhZYta_8TBU1"
      },
      "source": [
        "We can now see that we have two DataFrame, one containing only the movies and the other only the people which play as actor/actress in a title. To do the desired maket-basket analysis we have to pivot our tconst as rows, so each row stands for one titleId, and then including a list of nconst identifiers of the actors that played in it."
      ],
      "id": "AhZYta_8TBU1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg5cCJD1Swlg"
      },
      "source": [
        "df_basics.show(10)"
      ],
      "id": "Gg5cCJD1Swlg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BnEuTc0S9Xm"
      },
      "source": [
        "df_principals.show(10)"
      ],
      "id": "7BnEuTc0S9Xm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuwFSQN1VyCV"
      },
      "source": [
        "So we start by joining the two dataframe to extract from df_principals only the records with tconst related to a movie. We can also discard the category column since is no longer usefull."
      ],
      "id": "QuwFSQN1VyCV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81uUuBOVVj3a"
      },
      "source": [
        "basket_data = df_principals.join(df_basics, \"tconst\").select(df_principals.tconst, df_principals.nconst).sort(\"tconst\")"
      ],
      "id": "81uUuBOVVj3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX0u3lbuXzR-"
      },
      "source": [
        "basket_data.show(10)"
      ],
      "id": "iX0u3lbuXzR-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHywTOMEZbgu"
      },
      "source": [
        "Then we can remove hypothetical duplicated row and then aggregate the data using tconst identifier."
      ],
      "id": "xHywTOMEZbgu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYN6WQg5Vj4x"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "basket_data = basket_data.dropDuplicates()\n",
        "basket_data = basket_data.groupBy(\"tconst\").agg(F.collect_list(\"nconst\").alias(\"nconsts\")).sort('tconst')"
      ],
      "id": "IYN6WQg5Vj4x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZzSHEsFaa0G"
      },
      "source": [
        "print(\"There are {} titleId buckets\".format(basket_data.count()))\n",
        "basket_data.show(10, False)"
      ],
      "id": "IZzSHEsFaa0G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOFUnUEZclPs"
      },
      "source": [
        "As we can see above we now have the data in the correct format to do our analysis: in each row we have the identifier of a movie and in the second column the list of the idenfiers of the actors that played in it.\n",
        "Since we done all the needed pre-processing computation on the data we can transform our DataFrame in a RDD to apply map-reduce functions."
      ],
      "id": "JOFUnUEZclPs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PXtfoBH41dy"
      },
      "source": [
        "Serialize to file the RDD and download to skip the processing all the time.\n",
        "\n"
      ],
      "id": "4PXtfoBH41dy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poNWxUnLm92f"
      },
      "source": [
        "basket_data.write.format('json').save(\"data\")"
      ],
      "id": "poNWxUnLm92f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_0NCykYVZeB"
      },
      "source": [
        "!zip -r data.zip data"
      ],
      "id": "c_0NCykYVZeB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXXeT6F0Vg5q"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('data.zip')"
      ],
      "id": "MXXeT6F0Vg5q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti7KSSed4h34"
      },
      "source": [
        "# Apriori classic"
      ],
      "id": "ti7KSSed4h34"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTGVInR_j-Fu"
      },
      "source": [
        "We start by implementing the classic Apriori algorithm. In particular we search only for tuples and not larger itemsets."
      ],
      "id": "UTGVInR_j-Fu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSVGRVNaJaiC"
      },
      "source": [
        "from itertools import tee\n",
        "\n",
        "def apriori_jhvbjknsm(partitionData, support_threshold):\n",
        "  singleton_counter = {}\n",
        "  \n",
        "  d1, d2 = tee(partitionData, 2)\n",
        "\n",
        "  # count singletons\n",
        "  for _, bucket in d1:\n",
        "    for item in bucket:\n",
        "      singleton_counter[item] = singleton_counter.get(item, 0) + 1\n",
        "\n",
        "  frequent_singleton = [(k,v) for k,v in singleton_counter.items() if v >= support_threshold]\n",
        "\n",
        "  # count pairs\n",
        "  pair_counter = {}\n",
        "  for _, bucket in d2:\n",
        "      frequent_items_of_bucket = [item for item in bucket if singleton_counter.get(item, 0) >= support_threshold]\n",
        "      \n",
        "      for x in frequent_items_of_bucket:\n",
        "          for y in frequent_items_of_bucket:\n",
        "              if x<y:\n",
        "                  pair_counter[(x,y)] = pair_counter.get((x,y), 0) + 1\n",
        "\n",
        "  # tuple(sorted(couple)) is done because may a couple be generated backward in a partition\n",
        "  frequent_couples = [(sorted(couple), support) for couple, support in pair_counter.items() if support >= support_threshold]\n",
        "  \n",
        "  return iter(frequent_singleton + frequent_couples)"
      ],
      "id": "KSVGRVNaJaiC",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AghTTfDdxcAu"
      },
      "source": [
        "from itertools import tee\n",
        "\n",
        "def apriori(partitionData, support_threshold):\n",
        "  singleton_counter = []\n",
        "  lookup_index_table = {}\n",
        "  reverse_lookup_index_table = {}\n",
        "  \n",
        "  d1, d2 = tee(partitionData, 2)\n",
        "\n",
        "  # count singletons\n",
        "  for bucket in d1:\n",
        "    for item in bucket[1]:\n",
        "      if item not in lookup_index_table:\n",
        "        # The newly discovered element is appended on the tail of the array counter\n",
        "        lookup_index_table[item] = len(singleton_counter)\n",
        "        reverse_lookup_index_table[len(singleton_counter)] = item\n",
        "        singleton_counter.append(0)\n",
        "      idx = lookup_index_table[item]\n",
        "      singleton_counter[idx] += 1\n",
        "\n",
        "  frequent_items_table = [index for index,count in enumerate(singleton_counter) if count >= support_threshold]\n",
        "  frequent_singleton = [(reverse_lookup_index_table[item], singleton_counter[item]) for item in frequent_items_table]\n",
        "  # count pairs\n",
        "  pair_counter = {}\n",
        "  for bucket in d2:\n",
        "      frequent_items_of_bucket = [lookup_index_table[item] for item in bucket[1] \n",
        "                        if lookup_index_table[item] in frequent_items_table]\n",
        "      \n",
        "      for x in frequent_items_of_bucket:\n",
        "          for y in frequent_items_of_bucket:\n",
        "              if x<y:\n",
        "                  pair_counter[(x,y)] = pair_counter.get((x,y), 0) + 1\n",
        "\n",
        "  # tuple(sorted(couple)) is done because may a couple be generated backward in a partition\n",
        "  frequent_couples = [(tuple(sorted((reverse_lookup_index_table[couple[0]], reverse_lookup_index_table[couple[1]]))), count) for couple ,count \n",
        "                      in pair_counter.items() if count >= support_threshold]\n",
        "  \n",
        "  return iter(frequent_singleton + frequent_couples)"
      ],
      "id": "AghTTfDdxcAu",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7dZSxBd1hWQ"
      },
      "source": [
        "# Apriori with MAP-REDUCE"
      ],
      "id": "I7dZSxBd1hWQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYDq3kNh5aql"
      },
      "source": [
        "Follow an implementatio of the Apriori algorithm using a map-reduce approach. The logic in the implementation is a bit different than the one provided by the book, in particular in the approach of generating couples. Also in this case we stop our search to tuples of frequent itemsets."
      ],
      "id": "YYDq3kNh5aql"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z0pKUxbAgTF"
      },
      "source": [
        "def apriorihmap(data, support_threshold):\n",
        "    \"\"\" \n",
        "    data: Pyspark.rdd \n",
        "      [\n",
        "        [tconst, [nconst,]],\n",
        "      ]\n",
        "    \"\"\"\n",
        "    nconst_rdd = data.map(lambda x: x[1])\n",
        "\n",
        "    frequent_items_rdd = nconst_rdd.flatMap(lambda x: x) \\\n",
        "          .map(lambda elem: (elem,1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .filter(lambda x: x[1] >= support_threshold)\n",
        "\n",
        "    #print(f\"found {frequent_items_rdd.count()} frequent singletons\")\n",
        "    frequent_singletons_bv = spark.sparkContext.broadcast({k[0]:True for k in frequent_items_rdd.collect()})\n",
        "\n",
        "    def generate_candidate(x):\n",
        "      candidates = []\n",
        "      for a in x:\n",
        "        for b in x:\n",
        "          if a < b:\n",
        "            # the tuple may be generated backwards, sort to get rid of the problem\n",
        "            candidates.append((a,b))\n",
        "      return candidates\n",
        "    \n",
        "    frequent_couples_rdd = data.map(lambda x: x[1]) \\\n",
        "          .filter(lambda x: [elem for elem in x if frequent_singletons_bv.value.get(elem, False)])\\\n",
        "          .flatMap(lambda x: generate_candidate(x)) \\\n",
        "          .map(lambda x: (x,1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .filter(lambda x: x[1] >=support_threshold)\n",
        "\n",
        "    return frequent_items_rdd.union(frequent_couples_rdd)"
      ],
      "id": "1z0pKUxbAgTF",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KsAA5w_21g1"
      },
      "source": [
        "# SON"
      ],
      "id": "0KsAA5w_21g1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Kk0QGUlr-X"
      },
      "source": [
        "We then decided to also implement SON to test out if there is an improvement in time complexity. In the first step of the algorithm we decided to use the classic apriori implementation done before for the map function. We decided to partion the data in a number equal to the avaiable processors in the cluster."
      ],
      "id": "P2Kk0QGUlr-X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJgYWyXy28lB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f35c0b0-5268-4675-a419-532af7bdf249"
      },
      "source": [
        "# empirical sweet-spot for the number of partitions (assuming every executor has 4 cores ...)\n",
        "num_partitions = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().size() * 4\n",
        "num_partitions"
      ],
      "id": "vJgYWyXy28lB",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJwtNWu9mNL3"
      },
      "source": [
        "We must define a function for the second step to properly count the number of occurrence of frequent itemsets in a partition."
      ],
      "id": "eJwtNWu9mNL3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGizCiVdI01u"
      },
      "source": [
        "def count_in_partition(data, frequent):\n",
        "  # prepare data for processing\n",
        "  frequent = frequent.value   # extract broadcasted values\n",
        "  data = list(data)           # cast to list to iterate more than one time\n",
        "\n",
        "  # check foreach frequent itemset\n",
        "  for frequent_item in frequent:\n",
        "    # trick to cast single element to list → not remove in the str duplicate char using set()\n",
        "    if type(frequent_item) is not tuple:\n",
        "      to_check = [frequent_item]\n",
        "    else:\n",
        "      to_check = frequent_item\n",
        "      \n",
        "    c = 0     # counter\n",
        "    # and foreach row of the dataset\n",
        "    for itemset in data:\n",
        "      # check if the frequent itemset is subset of the items of the row\n",
        "      if set(to_check).issubset(itemset[1]):\n",
        "        c += 1\n",
        "    yield (frequent_item, c)"
      ],
      "id": "eGizCiVdI01u",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6lKtG-gpYk"
      },
      "source": [
        "def count_in_partition_v2(data, candidate_frequent_itemsets_bv):\n",
        "  # extract broadcasted values\n",
        "  candidate_frequent_itemsets = candidate_frequent_itemsets_bv.value\n",
        "\n",
        "  # check foreach frequent itemset\n",
        "  for candidate_freq_item in candidate_frequent_itemsets.keys():\n",
        "    # need candidate_freq_item to be iterable even if it's only a single element\n",
        "    if type(candidate_freq_item) is not tuple:\n",
        "      candidate_freq_item = [candidate_freq_item]\n",
        "      \n",
        "    c = 0\n",
        "    for _, bucket in data:\n",
        "      if set(candidate_freq_item).issubset([x for x in bucket if candidate_frequent_itemsets.get(x,False)]):\n",
        "        c += 1\n",
        "    yield (tuple(candidate_freq_item), c)"
      ],
      "id": "ff6lKtG-gpYk",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "565n3mcLm4vX"
      },
      "source": [
        "Then the implementation of SON with a two step map-reduce. The first finds out the frequent itemsets in the partition and the latter go to count them in the dataset and filters out the ones with support greater than threshold."
      ],
      "id": "565n3mcLm4vX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwaMIM_AI33k"
      },
      "source": [
        "def son_m_r(data, support):\n",
        "  reduced_support = support//data.getNumPartitions()\n",
        "  first_map = data.mapPartitions(lambda partition: apriori(partition, reduced_support)).map(lambda x: (x[0], 1))\n",
        "  first_reduce = first_map.reduceByKey(lambda a,b: a+b)       # possible to remove a+b ?????????????????\n",
        "\n",
        "  # extract the frequent items and broadcast them to worker nodes\n",
        "  frequent_items = [x[0] for x in first_reduce.collect()]\n",
        "  frequent_items = spark.sparkContext.broadcast(frequent_items)\n",
        "\n",
        "  second_map = data.mapPartitions(lambda partition: count_in_partition(partition, frequent_items))\n",
        "  second_reduce = second_map.reduceByKey(lambda a,b: a+b).filter(lambda x: x[1] >= support)\n",
        "  return second_reduce"
      ],
      "id": "BwaMIM_AI33k",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImC5plrpZ4wv"
      },
      "source": [
        "def son_m_r_v2(data, support):\n",
        "  reduced_support = support/data.getNumPartitions()  # mi sa che non si deve arrotondare. va bene se è un float altrimenti poterbbe uscire 0\n",
        "  candidate_frequent_itemsets_rdd = data.mapPartitions(lambda partition: apriori(partition, reduced_support)).map(lambda x: x[0]).distinct()\n",
        "\n",
        "  # broadcast the frequent items to worker nodes\n",
        "  candidate_frequent_itemsets_bv = spark.sparkContext.broadcast(\n",
        "      {x:True for x in candidate_frequent_itemsets_rdd.collect()}\n",
        "  )\n",
        "\n",
        "  second_map = data.mapPartitions(lambda partition: count_in_partition_v2(partition, candidate_frequent_itemsets_bv))\n",
        "  frequent_itemsets = second_map.reduceByKey(lambda a,b: a+b).filter(lambda x: x[1] >= support)\n",
        "  return frequent_itemsets"
      ],
      "id": "ImC5plrpZ4wv",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3NcLCryldUa"
      },
      "source": [
        "# Demo FP Growth"
      ],
      "id": "z3NcLCryldUa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nE0iULGnbS1"
      },
      "source": [
        "To carry our experiment we decided to also use the in library implementation of FP-growth as comparison benchmark."
      ],
      "id": "1nE0iULGnbS1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz6FTwiSlgtB"
      },
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "fpGrowth = FPGrowth(itemsCol=\"nconsts\")"
      ],
      "id": "Fz6FTwiSlgtB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka2nZvdMmjzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d497288a-11cf-4861-c4a8-0beb8b6943b0"
      },
      "source": [
        "\"\"\"\n",
        "model = fpGrowth.fit(basket_data)\n",
        "\n",
        "# Display frequent itemsets.\n",
        "model.freqItemsets.show()\n",
        "items = model.freqItemsets\n",
        "\n",
        "# Display generated association rules.\n",
        "model.associationRules.show()\n",
        "rules = model.associationRules\n",
        "\n",
        "# transform examines the input items against all the association rules and summarize the consequents as prediction\n",
        "model.transform(basket_data).show()\n",
        "transformed = model.transform(basket_data)\n",
        "\"\"\""
      ],
      "id": "Ka2nZvdMmjzL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmodel = fpGrowth.fit(basket_data)\\n\\n# Display frequent itemsets.\\nmodel.freqItemsets.show()\\nitems = model.freqItemsets\\n\\n# Display generated association rules.\\nmodel.associationRules.show()\\nrules = model.associationRules\\n\\n# transform examines the input items against all the association rules and summarize the consequents as prediction\\nmodel.transform(basket_data).show()\\ntransformed = model.transform(basket_data)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y3eYwlUhklx"
      },
      "source": [
        "# Test of the algorithms"
      ],
      "id": "6Y3eYwlUhklx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAmmpjQxh37m"
      },
      "source": [
        "We extract a subset of 500 rows from the dataset to test out that our algorithms work as expected. We define min_support as 1% of the count of the rows."
      ],
      "id": "MAmmpjQxh37m"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFU3nax7hwzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3f6c77-1ee2-4b45-b9c9-a17bcaa73b47"
      },
      "source": [
        "minsup = 0.01\n",
        "num_rows = 500\n",
        "sup = minsup*num_rows\n",
        "\n",
        "minid = data.take(num_rows)\n",
        "minid = spark.sparkContext.parallelize(minid)\n",
        "minid.take(5)"
      ],
      "id": "DFU3nax7hwzB",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(tconst='tt0009987', nconsts=['nm0543583', 'nm0681933', 'nm0533764', 'nm0014894', 'nm0330217', 'nm0356228']),\n",
              " Row(tconst='tt0009988', nconsts=['nm0026387', 'nm0130788', 'nm0326378', 'nm0448696', 'nm0116050', 'nm0600299', 'nm0213156']),\n",
              " Row(tconst='tt0009989', nconsts=['nm0097670', 'nm0585931', 'nm0231657', 'nm0212326']),\n",
              " Row(tconst='tt0009990', nconsts=['nm0700447', 'nm0522281', 'nm0249400', 'nm0591103', 'nm0601194', 'nm0191596']),\n",
              " Row(tconst='tt0009991', nconsts=['nm0338824', 'nm0712660', 'nm0550619', 'nm0855979', 'nm0683265'])]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ2qhpswIbYH",
        "outputId": "06c40f1e-f501-4416-bdb3-5c1815ba5225"
      },
      "source": [
        "!pip install memory_profiler\n",
        "%load_ext memory_profiler"
      ],
      "id": "mQ2qhpswIbYH",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=30190 sha256=40263b7dcc2251bf0c68ab8602f6027e576c375342a4502628a72b56f64dac23\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.58.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeweibBnjlt-"
      },
      "source": [
        "We start by exectuing the classic implementation of apriori. Is compulsory to before collect the data from the RDD since this is a non distributed implementation."
      ],
      "id": "UeweibBnjlt-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvT1hHJejqx-"
      },
      "source": [
        "apriori_classic = list(apriori(minid.collect(), sup))"
      ],
      "id": "bvT1hHJejqx-",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj2u9qUNMsXF"
      },
      "source": [
        "apriori_2 = apriori_ujhnsvlkfm(minid.collect(), sup)"
      ],
      "id": "uj2u9qUNMsXF",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJQ80_vgj8GV"
      },
      "source": [
        "The we have the Apriori implementation with map-reduce"
      ],
      "id": "uJQ80_vgj8GV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE7l29m1j7m9"
      },
      "source": [
        "%memit %timeit apriori_map = apriorihmap(minid, sup).collect()"
      ],
      "id": "TE7l29m1j7m9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLsmu3F9k71N"
      },
      "source": [
        "Follow the implementation with SON. The data must be repartioned on the finded sweet-spot number of partitions"
      ],
      "id": "NLsmu3F9k71N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx6QeQghMAXf"
      },
      "source": [
        "import time"
      ],
      "id": "dx6QeQghMAXf",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6IRRQKJ2rF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75b272c-c16b-4a5b-f79b-9343d1726637"
      },
      "source": [
        "minid = minid.repartition(num_partitions)\n",
        "b = time.time()\n",
        "son = son_m_r(minid,sup).collect()\n",
        "print(time.time()-b)"
      ],
      "id": "y6IRRQKJ2rF7",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.62368631362915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5BAUF1yUIrL9",
        "outputId": "46be4dd8-7c62-49ca-cb4d-de24e76ebc84"
      },
      "source": [
        "b = time.time()\n",
        "son_v2 = son_m_r_v2(minid, sup).collect()\n",
        "print(time.time()-b)"
      ],
      "id": "5BAUF1yUIrL9",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-4f34f05349aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mson_v2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mson_m_r_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-910343ef8d1c>\u001b[0m in \u001b[0;36mson_m_r_v2\u001b[0;34m(data, support)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# broadcast the frequent items to worker nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   candidate_frequent_itemsets_bv = spark.sparkContext.broadcast(\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidate_frequent_itemsets_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 82.0 failed 1 times, most recent failure: Lost task 1.0 in stage 82.0 (TID 357) (8ca0c4ee0dcd executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dl2cFhxoIp1"
      },
      "source": [
        "Then we also train the in-library implementation of FPGrowth"
      ],
      "id": "6Dl2cFhxoIp1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-JGaLC6oIL2"
      },
      "source": [
        "# initialize\n",
        "fpGrowth.setMinSupport(minsup)\n",
        "model = fpGrowth.fit(minid.toDF())\n",
        "\n",
        "# get itemsets\n",
        "fp_growth = model.freqItemsets.collect()"
      ],
      "id": "i-JGaLC6oIL2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdmS1h6zsGCd"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def trasform_format(data):\n",
        "  strings = []\n",
        "  tuples = []\n",
        "  for d in data:\n",
        "    if len(d[0]) == 1:\n",
        "      strings.append((d[0][0], d[1]))\n",
        "    else:\n",
        "      tuples.append((tuple(sorted(d[0])),d[1]))\n",
        "  return strings + tuples\n",
        "\n",
        "# keep only singleton and tuples in fpgrowth result and trasform the format of results\n",
        "fp_growth = [item for item in fp_growth if len(item[0]) <= 2]\n",
        "fp_growth = trasform_format(fp_growth)"
      ],
      "id": "KdmS1h6zsGCd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP1FgwrOwz5F"
      },
      "source": [
        "Let's put the obtained result in a tabular way."
      ],
      "id": "aP1FgwrOwz5F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOfyQmTHiyfM"
      },
      "source": [
        "df1 = pd.DataFrame([x[1] for x in apriori_classic], index=[x[0] for x in apriori_classic], columns =['Apriori classic'])\n",
        "df2 = pd.DataFrame([x[1] for x in apriori_map], index=[x[0] for x in apriori_map], columns =['Apriori map'])\n",
        "df3 = pd.DataFrame([x[1] for x in son], index=[x[0] for x in son], columns =['SON'])\n",
        "df4 = pd.DataFrame([x[1] for x in fp_growth], index=[x[0] for x in fp_growth], columns =['FPGrowth'])\n",
        "\n",
        "df = pd.concat([df1, df2, df3, df4], axis=1)            \n",
        "df"
      ],
      "id": "lOfyQmTHiyfM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXj2acThjYp9"
      },
      "source": [
        ""
      ],
      "id": "LXj2acThjYp9",
      "execution_count": null,
      "outputs": []
    }
  ]
}