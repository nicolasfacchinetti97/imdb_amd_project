{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "protecting-prague"
   },
   "source": [
    "# Project 2: Market-basket analysis - IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tough-romance"
   },
   "source": [
    "Project for the course of Algorithms for Massive Dataset <br> Nicolas Facchinetti 961648 <br> Antonio Belotti 960822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGRAllgArUcg"
   },
   "source": [
    "# Set up the Spark enviorment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1-A43y-raCw"
   },
   "source": [
    "We start by dowloading and installing all the needed tool to deal with Spark. In particular we are interested in obtainig a Java enviorment since Spark in written in Scala and so it need a JVM to run. Then we can download Apache Spark 3.1.2 with Hadoop 3.2 by the Apache CDN and uncompress it. Finally we can get and install PySpark, an interface for Apache Spark in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylCDuP_tsDRB"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3amH6p0nwb7I"
   },
   "source": [
    "The next step is to correctly set the path in our remote enviorment to use the obtained tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXdPB9pQvM6Y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMGP-hYawvcv"
   },
   "source": [
    "Finally we can import PySpark in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Imf5XE-w84G"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"spark-3.1.2-bin-hadoop3.2\")# SPARK_HOME\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arabic-forwarding"
   },
   "source": [
    "# Download the dataset from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cardiac-significance"
   },
   "source": [
    "First install the Python module of Kaggle to download the dataset from its datacenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "third-confidence"
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "attractive-recall"
   },
   "source": [
    "Then load kaggle.json, a file containing your API credentials to be able to use the services offered by Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "narrow-future"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "  \n",
    "# Move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "entitled-stanley"
   },
   "source": [
    "Now we can download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "built-indianapolis"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download 'ashirwadsangwan/imdb-dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naked-dinner"
   },
   "source": [
    "We now must unzip the compressed archive to use it. Once done we can also remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "every-homework"
   },
   "outputs": [],
   "source": [
    "!unzip imdb-dataset.zip && rm imdb-dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2wchK4S6FSd"
   },
   "source": [
    "# Preapare the data for Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkCLCyVf6P81"
   },
   "source": [
    "We can directly load the downloaded and extracted .tsv file in a Spark DataFrame by using the command read.csv(). We directly pass to the method the columns in which we are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9yN4DYO5rA9"
   },
   "outputs": [],
   "source": [
    "df_principals = spark.read.csv(\"/content/title.principals.tsv/title.principals.tsv\", sep=r'\\t', header=True).select('tconst','nconst','category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFS4-BghiEt0"
   },
   "outputs": [],
   "source": [
    "df_principals.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biVqVaIO6rm1"
   },
   "outputs": [],
   "source": [
    "df_basics = spark.read.csv(\"/content/title.basics.tsv/title.basics.tsv\", sep=r'\\t', header=True).select('tconst','titleType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVXn5-qxiGjL"
   },
   "outputs": [],
   "source": [
    "df_basics.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYEMZq6g8V8r"
   },
   "source": [
    "By inspecting the content of the column 'category' of df_principlas we can see that there are many jobs other than actors and actress (which are the two we are interested in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ANddVhG7dOM"
   },
   "outputs": [],
   "source": [
    "df_principals.select(\"category\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chgrEDU181jc"
   },
   "source": [
    "Similarly we can do the same thing with df_basics and the column 'titleType' to see how many categories a title can have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0k9A__I8uop"
   },
   "outputs": [],
   "source": [
    "df_basics.select(\"titleType\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRGTAAFK7QJ0"
   },
   "source": [
    "Once the data is loaded in a Spark DataFrame we can use the PySpark SQL module for processing the data. We start by exctracting only actors and actress from df_principals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wrOWYKr8UC3"
   },
   "outputs": [],
   "source": [
    "pre = df_principals.count()\n",
    "df_principals.createOrReplaceTempView(\"PRINCIPALS\") # create a temporary table on DataFrame\n",
    "df_principals = spark.sql(\"SELECT * from PRINCIPALS WHERE category ='actor' OR category='actress'\")\n",
    "print(\"We reduced the number of row from {} to {}\".format(pre, df_principals.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFE8PdWZ9UAa"
   },
   "source": [
    " And then we do the same thing with movies in df_basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7idYi-s9ZlZ"
   },
   "outputs": [],
   "source": [
    "pre = df_basics.count()\n",
    "df_basics.createOrReplaceTempView(\"BASICS\") # create a temporary table on DataFrame\n",
    "df_basics = spark.sql(\"SELECT * from BASICS WHERE titleType ='movie'\")\n",
    "print(\"We reduced the number of row from {} to {}\".format(pre, df_basics.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhZYta_8TBU1"
   },
   "source": [
    "We can now see that we have two DataFrame, one containing only the movies and the other only the people which play as actor/actress in a title. To do the desired maket-basket analysis we have to pivot our tconst as rows, so each row stands for one titleId, and then including a list of nconst identifiers of the actors that played in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg5cCJD1Swlg"
   },
   "outputs": [],
   "source": [
    "df_basics.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BnEuTc0S9Xm"
   },
   "outputs": [],
   "source": [
    "df_principals.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuwFSQN1VyCV"
   },
   "source": [
    "So we start by joining the two dataframe to extract from df_principals only the records with tconst related to a movie. We can also discard the category column since is no longer usefull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81uUuBOVVj3a"
   },
   "outputs": [],
   "source": [
    "basket_data = df_principals.join(df_basics, \"tconst\").select(df_principals.tconst, df_principals.nconst).sort(\"tconst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iX0u3lbuXzR-"
   },
   "outputs": [],
   "source": [
    "basket_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHywTOMEZbgu"
   },
   "source": [
    "Then we can remove hypothetical duplicated row and then aggregate the data using tconst identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYN6WQg5Vj4x"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "basket_data = basket_data.dropDuplicates()\n",
    "basket_data = basket_data.groupBy(\"tconst\").agg(F.collect_list(\"nconst\").alias(\"nconsts\")).sort('tconst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZzSHEsFaa0G"
   },
   "outputs": [],
   "source": [
    "print(\"There are {} titleId buckets\".format(basket_data.count()))\n",
    "basket_data.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOFUnUEZclPs"
   },
   "source": [
    "As we can see above we now have the data in the correct format to do our analysis: in each row we have the identifier of a movie and in the second column the list of the idenfiers of the actors that played in it.\n",
    "Since we done all the needed pre-processing computation on the data we can transform our DataFrame in a RDD to apply map-reduce functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PXtfoBH41dy"
   },
   "source": [
    "Serialize to file the RDD and download to skip the processing all the time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poNWxUnLm92f"
   },
   "outputs": [],
   "source": [
    "basket_data.write.format('json').save(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_0NCykYVZeB"
   },
   "outputs": [],
   "source": [
    "!zip -r data.zip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXXeT6F0Vg5q"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('data.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_yuGgToVZK8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7dZSxBd1hWQ"
   },
   "source": [
    "# APRIORI MAP-REDUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stADeaUjnbVT"
   },
   "outputs": [],
   "source": [
    "data = spark.read.format(\"json\").option(\"header\", \"true\").load(\"data\").select('tconst', 'nconsts').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uzteeM5n2iL"
   },
   "outputs": [],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJZitqn3Ei6F"
   },
   "source": [
    "Accediamo al campo 1 sicchè 0 è il bucket, flat perché cosi unisce tutte le row in una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mnuJQJyDP81"
   },
   "outputs": [],
   "source": [
    "data.flatMap(lambda row: row[1]).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vfem5sDEo9r"
   },
   "source": [
    "Mappiamo ogni record di autore trovato in se stesso e 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PgqCjGtEoVn"
   },
   "outputs": [],
   "source": [
    "data.flatMap(lambda row: (row[1]))\n",
    "  .map(lambda elem: (elem,1)).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t07IEegHFRvJ"
   },
   "source": [
    "Aggiungiamo reduce che somma la parte dopo il contantore dell'attore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMu-bK-yFTcT"
   },
   "outputs": [],
   "source": [
    "data.flatMap(lambda row: (row[1])).map(lambda elem: (elem,1)).reduceByKey(lambda a,b: a+b).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuPjOzl_GCGc"
   },
   "source": [
    "Aggiungiamo un threshold (almeno 200 apparizioni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bd_UXlGHYAO"
   },
   "outputs": [],
   "source": [
    "res = data.flatMap(lambda row: (row[1])) \\\n",
    "          .map(lambda elem: (elem,1)) \\\n",
    "          .reduceByKey(lambda a,b: a+b) \\\n",
    "          .filter(lambda x: x[1] >=200)\n",
    "res.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvPvtIrGMN0M"
   },
   "source": [
    "Vediamo ora per la seconda parte di apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-he96ytnMQVj"
   },
   "outputs": [],
   "source": [
    "data.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UGC-sTeP-R7"
   },
   "source": [
    "Prendiamo il primo record per provare e estriamo i due elementi. Scriviamo una funzione che controlla se gli elementi di una copia sono nella riga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jm22mtPROAXc"
   },
   "outputs": [],
   "source": [
    "coppia = ['nm0063086', 'nm0183823']    #primi due attori del primo record\n",
    "\n",
    "def row_contains_elements(row, elements):\n",
    "  return all(x in row for x in elements)\n",
    "\n",
    "data.map(lambda x:x[1]).filter(lambda x: row_contains_elements(x,coppia)).take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVckVsR_xDWP"
   },
   "source": [
    "Proviamo ora a cercare di far generare le copie possibili ad ogni singola riga. trick per evitare doppioni. flatmap direttamente almeno sono gia spacchettate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u96RReA1hh9"
   },
   "outputs": [],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gk-9ihyWoxh_"
   },
   "outputs": [],
   "source": [
    "def generate_candidate(x):\n",
    "  candidates = []\n",
    "  for a, elemA in enumerate(x):\n",
    "    for b, elemB in enumerate(x):\n",
    "      if a < b:\n",
    "        candidates.append((elemA, elemB))\n",
    "  return candidates\n",
    "\n",
    "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTp40IkixO4x"
   },
   "source": [
    "Aggiungiamo poi un controllo che la copia generata sia in quelle di interesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w75-jNucwKwQ"
   },
   "outputs": [],
   "source": [
    "copia = [('nm0063086', 'nm0183823'), ('nm0846894', 'nm3002376')]\n",
    "\n",
    "def generate_candidate(x):\n",
    "  candidates = []\n",
    "  for a, elemA in enumerate(x):\n",
    "    for b, elemB in enumerate(x):\n",
    "      if a < b:\n",
    "        candidates.append((elemA, elemB))\n",
    "  return candidates\n",
    "\n",
    "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)).filter(lambda x: x in copia).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxuUOII3xV_w"
   },
   "source": [
    "Vero proprio passo di map. Le tuple per qualche motivo sono hashabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cK5xVYz1xU2W"
   },
   "outputs": [],
   "source": [
    "copia = [('nm0063086', 'nm0183823'), ('nm0846894', 'nm3002376')]\n",
    "\n",
    "def generate_candidate(x):\n",
    "  candidates = []\n",
    "  for a, elemA in enumerate(x):\n",
    "    for b, elemB in enumerate(x):\n",
    "      if a < b:\n",
    "        candidates.append((elemA, elemB))\n",
    "  return candidates\n",
    "\n",
    "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)) \\\n",
    "    .filter(lambda x: x in copia).map(lambda x: (x,1)).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-ykb2KwxVGu"
   },
   "source": [
    "Aggiungiamo reduce e il controllo del threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOWrjKjYxCAG"
   },
   "outputs": [],
   "source": [
    "copia = [('nm0063086', 'nm0183823'), ('nm0846894', 'nm3002376')]\n",
    "\n",
    "def generate_candidate(x):\n",
    "  candidates = []\n",
    "  for a, elemA in enumerate(x):\n",
    "    for b, elemB in enumerate(x):\n",
    "      if a < b:\n",
    "        candidates.append((elemA, elemB))\n",
    "  return candidates\n",
    "\n",
    "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)) \\\n",
    "    .filter(lambda x: x in copia) \\\n",
    "    .map(lambda x: (x,1)) \\\n",
    "    .reduceByKey(lambda a,b: a+b) \\\n",
    "    .filter(lambda x: x[1] >=1) \\\n",
    "    .take(3)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1z0pKUxbAgTF"
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "def apriorihmap(data, support_threshold):\n",
    "    \"\"\" \n",
    "    data: Pyspark.rdd \n",
    "      [\n",
    "        [tconst, [nconst,]],\n",
    "      ]\n",
    "    \"\"\"\n",
    "    nconst_rdd = data.map(lambda x: x[1])\n",
    "\n",
    "    frequent_items_rdd = nconst_rdd.flatMap(lambda x: x) \\\n",
    "          .map(lambda elem: (elem,1)) \\\n",
    "          .reduceByKey(lambda a,b: a+b) \\\n",
    "          .filter(lambda x: x[1] >= support_threshold)\\\n",
    "          .map(lambda x: x[0])\n",
    "\n",
    "    def generate_candidate(x):\n",
    "      candidates = []\n",
    "      for a, elemA in enumerate(x):\n",
    "        for b, elemB in enumerate(x):\n",
    "          if a < b:\n",
    "            candidates.append((elemA, elemB))\n",
    "      return candidates\n",
    "\n",
    "    #print(f\"found {frequent_items_rdd.count()} frequent singletons\")\n",
    "    frequent_singletons_bv = sc.broadcast({k:True for k in frequent_items_rdd.collect()})\n",
    "\n",
    "    return data.map(lambda x: x[1]) \\\n",
    "          .filter(lambda x: [elem for elem in x if frequent_singletons_bv.value.get(elem, False)])\\\n",
    "          .flatMap(lambda x: generate_candidate(x)) \\\n",
    "          .map(lambda x: (x,1)) \\\n",
    "          .reduceByKey(lambda a,b: a+b) \\\n",
    "          .filter(lambda x: x[1] >=support_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-Ac0zO_BeYT"
   },
   "outputs": [],
   "source": [
    "rules = apriori(data, 60)\n",
    "rules.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KsAA5w_21g1"
   },
   "source": [
    "# SON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TETfE0GIDHq"
   },
   "outputs": [],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJfxeejf2xw5"
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "data = spark.read.parquet(\"data.parquet\").rdd #format(\"json\").option(\"header\", \"true\").load(\"data\").select('tconst', 'nconsts').rdd.flatMap(lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJgYWyXy28lB"
   },
   "outputs": [],
   "source": [
    "# empirical sweet-spot for the number of partitions (assuming every executor has 4 cores ...)\n",
    "num_partitions = sc._jsc.sc().getExecutorMemoryStatus().size() * 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4e2Xdbf4eZy"
   },
   "outputs": [],
   "source": [
    "# force actual partitioning. If you don't introduce a fake partition-key, the reduceByKey you do when counting elements will count the elements globally.\n",
    "\n",
    "support_threshold = 150\n",
    "\n",
    "import random\n",
    "data.map(lambda x: ((random.randint(1, num_partitions), x), 1))\\\n",
    "    .reduceByKey(lambda x,y: x+y)\\\n",
    "    .filter(lambda x: x[1] >= support_threshold / num_partitions)\\\n",
    "    .map(lambda x: (x[0][1], x[1]))\\\n",
    "    .reduceByKey(lambda x,y: x+y)\\\n",
    "    .filter(lambda x: x[1] >= support_threshold)\\\n",
    "    .first()\n",
    "\n",
    "\n",
    "    #TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3NcLCryldUa"
   },
   "source": [
    "# Demo FP Growht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fz6FTwiSlgtB"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "fpGrowth = FPGrowth(itemsCol=\"nconsts\", minSupport=0.0001, minConfidence=0.0001)\n",
    "model = fpGrowth.fit(basket_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oM56a8Jmjnb"
   },
   "outputs": [],
   "source": [
    "# Display frequent itemsets.\n",
    "model.freqItemsets.show()\n",
    "items = model.freqItemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ka2nZvdMmjzL"
   },
   "outputs": [],
   "source": [
    "# Display generated association rules.\n",
    "model.associationRules.show()\n",
    "rules = model.associationRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hnnm_GJMmj4h"
   },
   "outputs": [],
   "source": [
    "# transform examines the input items against all the association rules and summarize the consequents as prediction\n",
    "model.transform(basket_data).show()\n",
    "transformed = model.transform(basket_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6IRRQKJ2rF7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Acn29Pgj2rY3"
   },
   "source": [
    "# Demo Antonio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhiddpa3kiQR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDn6fZxXA6Fs"
   },
   "source": [
    "Lets try to load some data in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EB6Fpgfa1El1"
   },
   "outputs": [],
   "source": [
    "actors_cols = {\n",
    "    \"original\": [\n",
    "        \"nconst\",  # actor unique id\n",
    "        \"knownForTitles\"  # move he/she is in\n",
    "    ],\n",
    "    \"renamed\": [\"actorId\", \"titles\"]\n",
    "}\n",
    "\n",
    "actors_df = pd.read_csv(\n",
    "    \"name.basics.tsv.gz\",\n",
    "    compression=\"gzip\",\n",
    "    sep='\\t',\n",
    "    usecols=actors_cols[\"original\"]\n",
    ")\n",
    "\n",
    "# clean and pre-process\n",
    "actors_df.columns = actors_cols[\"renamed\"]\n",
    "actors_df.drop(actors_df[actors_df.titles == \"\\\\N\"].index, inplace=True)\n",
    "actors_df.titles = actors_df.titles.apply(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqBeMA_JqHQS"
   },
   "outputs": [],
   "source": [
    "actors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ddwv9h4kiQR"
   },
   "outputs": [],
   "source": [
    "def apriori(transactions, support_threshold):\n",
    "    singleton_counter = []\n",
    "    lookup_index_table = {}\n",
    "    reverse_lookup_index_table = {}\n",
    "\n",
    "    # count singletons\n",
    "    for bucket in transactions:\n",
    "        for elem in bucket:\n",
    "            if elem not in lookup_index_table:\n",
    "                # The newly discovered element is appended on the tail of the array counter\n",
    "                lookup_index_table[elem] = len(singleton_counter)\n",
    "                reverse_lookup_index_table[len(singleton_counter)] = elem\n",
    "                singleton_counter.append(0)\n",
    "\n",
    "            idx = lookup_index_table[elem]\n",
    "            singleton_counter[idx] += 1\n",
    "\n",
    "    frequent_items_table = [i for i,v in enumerate(singleton_counter) if v > support_threshold]\n",
    "\n",
    "    # count pairs\n",
    "    pair_counter = {}\n",
    "    for bucket in transactions:\n",
    "        frequent_items = [lookup_index_table[item] for item in bucket \n",
    "                          if lookup_index_table[item] in frequent_items_table]\n",
    "\n",
    "        for x in frequent_items:\n",
    "            for y in frequent_items:\n",
    "                if x<y:\n",
    "                    pair_counter[(x,y)] = pair_counter.get((x,y), 0) +1 \n",
    "\n",
    "    return [list(map(lambda x: reverse_lookup_index_table[x], i)) for i,c in pair_counter.items() \n",
    "            if c > support_threshold] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIQAUaiDkiQS"
   },
   "outputs": [],
   "source": [
    "# test\n",
    "rules = apriori(actors_df.titles, 300)\n",
    "\n",
    "movies_df = pd.read_csv(\"title.basics.tsv.gz\", compression='gzip', sep='\\t')\n",
    "from IPython.display import display\n",
    "\n",
    "for x,y in rules:\n",
    "    display(movies_df.loc[((movies_df.tconst == x) | (movies_df.tconst == y))])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lGRAllgArUcg",
    "z3NcLCryldUa",
    "Acn29Pgj2rY3"
   ],
   "name": "Copy of imdb_market_basket_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
