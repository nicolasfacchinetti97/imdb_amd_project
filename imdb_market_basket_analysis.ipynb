{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Copy of imdb_market_basket_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lGRAllgArUcg",
        "z3NcLCryldUa",
        "Acn29Pgj2rY3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "protecting-prague"
      },
      "source": [
        "# Project 2: Market-basket analysis - IMDB dataset"
      ],
      "id": "protecting-prague"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tough-romance"
      },
      "source": [
        "Project for the course of Algorithms for Massive Dataset <br> Nicolas Facchinetti 961648 <br> Antonio Belotti 960822"
      ],
      "id": "tough-romance"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJMEDFZ34-i3"
      },
      "source": [
        "# Load preprocessed dataset from file data.zip"
      ],
      "id": "nJMEDFZ34-i3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdFAFOdx5Q6n"
      },
      "source": [
        "Use the code below do load the dataset from a preprocessed file data.zip"
      ],
      "id": "LdFAFOdx5Q6n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ0o6v56gAGE",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ddd55ef6-07f7-414c-d7ef-9bd5a176bdfa"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "  \n",
        "!unzip data.zip && rm data.zip"
      ],
      "id": "kQ0o6v56gAGE",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-599695f7-723a-4b68-b718-5ab802192f6c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-599695f7-723a-4b68-b718-5ab802192f6c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.zip to data.zip\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/part-00063-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00174-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00180-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00071-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00187-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00017-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00062-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00140-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00114-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00003-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00091-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00138-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00087-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00178-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00006-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00160-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00076-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00186-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00166-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00070-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00004-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00192-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00129-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00196-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00144-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00173-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00027-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00117-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00142-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00190-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00122-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00034-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00134-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00081-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00127-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00184-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00021-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00163-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00169-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00116-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00064-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00160-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00193-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00012-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00105-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00094-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00098-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00198-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00053-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00123-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00024-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00162-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00133-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00004-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00127-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00007-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00054-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00054-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00002-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00176-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00183-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00089-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00073-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00018-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00109-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00107-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00196-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00111-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00005-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00115-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00003-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/_SUCCESS           \n",
            " extracting: data/.part-00099-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00036-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00153-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00034-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00115-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00048-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00052-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00104-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00113-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00062-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00194-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00116-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00155-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00016-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00065-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00142-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00140-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00199-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00164-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00198-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00193-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00166-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00162-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00111-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00188-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00172-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00154-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00044-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00112-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00188-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00081-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00072-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00063-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00046-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00028-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00096-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00011-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00179-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00048-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00100-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00090-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00152-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00110-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00112-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00124-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00010-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00156-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00195-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00175-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00149-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00066-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00031-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00130-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00051-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00152-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00061-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00061-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00086-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00163-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00045-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00094-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00008-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00056-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00015-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00189-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00013-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00133-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00085-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00135-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00056-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00009-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00172-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00139-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00097-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00107-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00046-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00068-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00175-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00099-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00113-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00082-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00136-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00021-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00171-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00035-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00122-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00097-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00040-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00156-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00164-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00157-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00020-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00057-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00110-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00019-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00143-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00114-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00095-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00083-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00157-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00145-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00182-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00027-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00105-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00120-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00093-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00088-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00159-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00042-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00076-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00150-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00155-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00037-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00033-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00008-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00000-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00070-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00080-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00072-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00095-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00128-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00007-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00168-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00190-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00158-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00138-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00123-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00030-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/._SUCCESS.crc      \n",
            " extracting: data/.part-00108-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00096-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00185-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00014-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00089-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00143-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00059-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00102-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00100-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00186-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00147-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00117-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00184-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00146-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00191-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00102-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00055-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00151-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00084-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00036-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00043-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00039-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00170-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00025-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00167-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00029-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00174-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00015-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00025-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00146-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00104-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00083-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00148-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00139-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00161-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00125-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00005-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00165-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00012-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00101-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00173-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00001-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00041-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00009-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00085-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00137-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00047-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00038-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00031-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00060-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00101-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00026-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00176-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00020-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00108-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00199-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00029-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00075-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00118-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00171-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00016-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00165-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00023-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00022-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00182-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00177-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00032-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00119-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00066-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00181-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00141-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00197-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00185-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00043-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00120-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00065-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00103-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00148-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00147-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00154-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00019-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00069-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00041-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00187-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00086-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00000-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00079-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00169-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00028-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00132-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00030-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00055-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00197-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00145-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00052-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00064-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00057-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00118-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00180-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00098-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00177-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00124-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00134-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00135-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00093-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00001-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00073-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00136-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00051-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00106-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00011-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00121-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00091-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00079-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00045-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00018-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00023-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00074-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00090-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00077-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00050-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00080-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00074-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00059-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00149-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00060-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00129-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00037-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00002-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00039-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00181-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00042-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00158-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00022-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00109-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00014-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00087-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00151-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00167-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00121-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00038-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00119-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00125-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00150-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00126-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00084-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00049-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00159-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00144-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00106-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00131-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00077-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00130-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00131-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00153-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00103-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00168-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00026-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00049-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00053-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00006-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00033-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00128-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00170-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00010-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00078-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00013-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00032-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00071-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00067-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00040-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00067-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00069-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00024-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00068-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00183-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00047-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00161-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00192-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00141-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00179-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00050-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00058-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00035-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00078-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00082-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00088-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00044-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00092-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00189-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00191-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00126-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00092-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00137-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00075-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00017-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            " extracting: data/.part-00178-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            " extracting: data/.part-00132-717f407c-435f-4def-bca7-0ae425d828a4-c000.json.crc  \n",
            "  inflating: data/part-00058-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00194-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n",
            "  inflating: data/part-00195-717f407c-435f-4def-bca7-0ae425d828a4-c000.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stADeaUjnbVT"
      },
      "source": [
        "data = spark.read.format(\"json\").option(\"header\", \"true\").load(\"data\").select('tconst', 'nconsts').rdd"
      ],
      "id": "stADeaUjnbVT",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uzteeM5n2iL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a4c5d85-7f1c-41e5-f718-e25a68a77836"
      },
      "source": [
        "data.take(5)"
      ],
      "id": "6uzteeM5n2iL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(tconst='tt0000009', nconsts=['nm0063086', 'nm0183823', 'nm1309758']),\n",
              " Row(tconst='tt0000335', nconsts=['nm1010955', 'nm1012612', 'nm1011210', 'nm1012621', 'nm0675239', 'nm0675260']),\n",
              " Row(tconst='tt0000502', nconsts=['nm0215752', 'nm0252720']),\n",
              " Row(tconst='tt0000574', nconsts=['nm0846887', 'nm0846894', 'nm3002376', 'nm0170118']),\n",
              " Row(tconst='tt0000615', nconsts=['nm3071427', 'nm0581353', 'nm0888988', 'nm0240418', 'nm0346387', 'nm0218953'])]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGRAllgArUcg"
      },
      "source": [
        "# Set up the Spark enviorment"
      ],
      "id": "lGRAllgArUcg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1-A43y-raCw"
      },
      "source": [
        "We start by dowloading and installing all the needed tool to deal with Spark. In particular we are interested in obtainig a Java enviorment since Spark in written in Scala and so it need a JVM to run. Then we can download Apache Spark 3.1.2 with Hadoop 3.2 by the Apache CDN and uncompress it. Finally we can get and install PySpark, an interface for Apache Spark in Python"
      ],
      "id": "G1-A43y-raCw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylCDuP_tsDRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7518823-d42b-40f3-cdd5-644d9119a97f"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "id": "ylCDuP_tsDRB",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-07 12:58:09--  https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228834641 (218M) [application/x-gzip]\n",
            "Saving to: spark-3.1.2-bin-hadoop3.2.tgz\n",
            "\n",
            "spark-3.1.2-bin-had 100%[===================>] 218.23M   183MB/s    in 1.2s    \n",
            "\n",
            "2021-09-07 12:58:11 (183 MB/s) - spark-3.1.2-bin-hadoop3.2.tgz saved [228834641/228834641]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3amH6p0nwb7I"
      },
      "source": [
        "The next step is to correctly set the path in our remote enviorment to use the obtained tools."
      ],
      "id": "3amH6p0nwb7I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXdPB9pQvM6Y"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "id": "EXdPB9pQvM6Y",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMGP-hYawvcv"
      },
      "source": [
        "Finally we can import PySpark in the project"
      ],
      "id": "IMGP-hYawvcv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Imf5XE-w84G"
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-3.1.2-bin-hadoop3.2\")# SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "id": "2Imf5XE-w84G",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arabic-forwarding"
      },
      "source": [
        "# Download the dataset from Kaggle"
      ],
      "id": "arabic-forwarding"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cardiac-significance"
      },
      "source": [
        "First install the Python module of Kaggle to download the dataset from its datacenter"
      ],
      "id": "cardiac-significance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "third-confidence"
      },
      "source": [
        "!pip install kaggle"
      ],
      "id": "third-confidence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attractive-recall"
      },
      "source": [
        "Then load kaggle.json, a file containing your API credentials to be able to use the services offered by Kaggle"
      ],
      "id": "attractive-recall"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "narrow-future"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "  \n",
        "# Move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "id": "narrow-future",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "entitled-stanley"
      },
      "source": [
        "Now we can download the dataset"
      ],
      "id": "entitled-stanley"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "built-indianapolis"
      },
      "source": [
        "!kaggle datasets download 'ashirwadsangwan/imdb-dataset'"
      ],
      "id": "built-indianapolis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naked-dinner"
      },
      "source": [
        "We now must unzip the compressed archive to use it. Once done we can also remove it."
      ],
      "id": "naked-dinner"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "every-homework"
      },
      "source": [
        "!unzip imdb-dataset.zip && rm imdb-dataset.zip"
      ],
      "id": "every-homework",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2wchK4S6FSd"
      },
      "source": [
        "# Preapare the data for Spark"
      ],
      "id": "B2wchK4S6FSd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkCLCyVf6P81"
      },
      "source": [
        "We can directly load the downloaded and extracted .tsv file in a Spark DataFrame by using the command read.csv(). We directly pass to the method the columns in which we are interested."
      ],
      "id": "vkCLCyVf6P81"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9yN4DYO5rA9"
      },
      "source": [
        "df_principals = spark.read.csv(\"/content/title.principals.tsv/title.principals.tsv\", sep=r'\\t', header=True).select('tconst','nconst','category')"
      ],
      "id": "o9yN4DYO5rA9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFS4-BghiEt0"
      },
      "source": [
        "df_principals.show(10)"
      ],
      "id": "TFS4-BghiEt0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biVqVaIO6rm1"
      },
      "source": [
        "df_basics = spark.read.csv(\"/content/title.basics.tsv/title.basics.tsv\", sep=r'\\t', header=True).select('tconst','titleType')"
      ],
      "id": "biVqVaIO6rm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVXn5-qxiGjL"
      },
      "source": [
        "df_basics.show(10)"
      ],
      "id": "tVXn5-qxiGjL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYEMZq6g8V8r"
      },
      "source": [
        "By inspecting the content of the column 'category' of df_principlas we can see that there are many jobs other than actors and actress (which are the two we are interested in)"
      ],
      "id": "jYEMZq6g8V8r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ANddVhG7dOM"
      },
      "source": [
        "df_principals.select(\"category\").distinct().show()"
      ],
      "id": "_ANddVhG7dOM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chgrEDU181jc"
      },
      "source": [
        "Similarly we can do the same thing with df_basics and the column 'titleType' to see how many categories a title can have."
      ],
      "id": "chgrEDU181jc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0k9A__I8uop"
      },
      "source": [
        "df_basics.select(\"titleType\").distinct().show()"
      ],
      "id": "H0k9A__I8uop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRGTAAFK7QJ0"
      },
      "source": [
        "Once the data is loaded in a Spark DataFrame we can use the PySpark SQL module for processing the data. We start by exctracting only actors and actress from df_principals"
      ],
      "id": "WRGTAAFK7QJ0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wrOWYKr8UC3"
      },
      "source": [
        "pre = df_principals.count()\n",
        "df_principals.createOrReplaceTempView(\"PRINCIPALS\") # create a temporary table on DataFrame\n",
        "df_principals = spark.sql(\"SELECT * from PRINCIPALS WHERE category ='actor' OR category='actress'\")\n",
        "print(\"We reduced the number of row from {} to {}\".format(pre, df_principals.count()))"
      ],
      "id": "0wrOWYKr8UC3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFE8PdWZ9UAa"
      },
      "source": [
        " And then we do the same thing with movies in df_basics"
      ],
      "id": "QFE8PdWZ9UAa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7idYi-s9ZlZ"
      },
      "source": [
        "pre = df_basics.count()\n",
        "df_basics.createOrReplaceTempView(\"BASICS\") # create a temporary table on DataFrame\n",
        "df_basics = spark.sql(\"SELECT * from BASICS WHERE titleType ='movie'\")\n",
        "print(\"We reduced the number of row from {} to {}\".format(pre, df_basics.count()))"
      ],
      "id": "_7idYi-s9ZlZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhZYta_8TBU1"
      },
      "source": [
        "We can now see that we have two DataFrame, one containing only the movies and the other only the people which play as actor/actress in a title. To do the desired maket-basket analysis we have to pivot our tconst as rows, so each row stands for one titleId, and then including a list of nconst identifiers of the actors that played in it."
      ],
      "id": "AhZYta_8TBU1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg5cCJD1Swlg"
      },
      "source": [
        "df_basics.show(10)"
      ],
      "id": "Gg5cCJD1Swlg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BnEuTc0S9Xm"
      },
      "source": [
        "df_principals.show(10)"
      ],
      "id": "7BnEuTc0S9Xm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuwFSQN1VyCV"
      },
      "source": [
        "So we start by joining the two dataframe to extract from df_principals only the records with tconst related to a movie. We can also discard the category column since is no longer usefull."
      ],
      "id": "QuwFSQN1VyCV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81uUuBOVVj3a"
      },
      "source": [
        "basket_data = df_principals.join(df_basics, \"tconst\").select(df_principals.tconst, df_principals.nconst).sort(\"tconst\")"
      ],
      "id": "81uUuBOVVj3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX0u3lbuXzR-"
      },
      "source": [
        "basket_data.show(10)"
      ],
      "id": "iX0u3lbuXzR-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHywTOMEZbgu"
      },
      "source": [
        "Then we can remove hypothetical duplicated row and then aggregate the data using tconst identifier."
      ],
      "id": "xHywTOMEZbgu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYN6WQg5Vj4x"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "basket_data = basket_data.dropDuplicates()\n",
        "basket_data = basket_data.groupBy(\"tconst\").agg(F.collect_list(\"nconst\").alias(\"nconsts\")).sort('tconst')"
      ],
      "id": "IYN6WQg5Vj4x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZzSHEsFaa0G"
      },
      "source": [
        "print(\"There are {} titleId buckets\".format(basket_data.count()))\n",
        "basket_data.show(10, False)"
      ],
      "id": "IZzSHEsFaa0G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOFUnUEZclPs"
      },
      "source": [
        "As we can see above we now have the data in the correct format to do our analysis: in each row we have the identifier of a movie and in the second column the list of the idenfiers of the actors that played in it.\n",
        "Since we done all the needed pre-processing computation on the data we can transform our DataFrame in a RDD to apply map-reduce functions."
      ],
      "id": "JOFUnUEZclPs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PXtfoBH41dy"
      },
      "source": [
        "Serialize to file the RDD and download to skip the processing all the time.\n",
        "\n"
      ],
      "id": "4PXtfoBH41dy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poNWxUnLm92f"
      },
      "source": [
        "basket_data.write.format('json').save(\"data\")"
      ],
      "id": "poNWxUnLm92f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_0NCykYVZeB"
      },
      "source": [
        "!zip -r data.zip data"
      ],
      "id": "c_0NCykYVZeB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXXeT6F0Vg5q"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('data.zip')"
      ],
      "id": "MXXeT6F0Vg5q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_yuGgToVZK8"
      },
      "source": [
        "# Prove Map-reduce"
      ],
      "id": "V_yuGgToVZK8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJZitqn3Ei6F"
      },
      "source": [
        "Accediamo al campo 1 sicch 0  il bucket, flat perch cosi unisce tutte le row in una"
      ],
      "id": "oJZitqn3Ei6F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mnuJQJyDP81"
      },
      "source": [
        "data.flatMap(lambda row: row[1]).take(10)"
      ],
      "id": "4mnuJQJyDP81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vfem5sDEo9r"
      },
      "source": [
        "Mappiamo ogni record di autore trovato in se stesso e 1"
      ],
      "id": "2vfem5sDEo9r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PgqCjGtEoVn"
      },
      "source": [
        "data.flatMap(lambda row: (row[1]))\n",
        "  .map(lambda elem: (elem,1)).take(10)"
      ],
      "id": "1PgqCjGtEoVn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t07IEegHFRvJ"
      },
      "source": [
        "Aggiungiamo reduce che somma la parte dopo il contantore dell'attore"
      ],
      "id": "t07IEegHFRvJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMu-bK-yFTcT"
      },
      "source": [
        "data.flatMap(lambda row: (row[1])).map(lambda elem: (elem,1)).reduceByKey(lambda a,b: a+b).take(10)"
      ],
      "id": "UMu-bK-yFTcT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuPjOzl_GCGc"
      },
      "source": [
        "Aggiungiamo un threshold (almeno 200 apparizioni)"
      ],
      "id": "ZuPjOzl_GCGc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bd_UXlGHYAO"
      },
      "source": [
        "res = data.flatMap(lambda row: (row[1])) \\\n",
        "          .map(lambda elem: (elem,1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .filter(lambda x: x[1] >=200)\n",
        "res.take(10)"
      ],
      "id": "6bd_UXlGHYAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvPvtIrGMN0M"
      },
      "source": [
        "Vediamo ora per la seconda parte di apriori"
      ],
      "id": "JvPvtIrGMN0M"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-he96ytnMQVj"
      },
      "source": [
        "data.take(10)"
      ],
      "id": "-he96ytnMQVj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UGC-sTeP-R7"
      },
      "source": [
        "Prendiamo il primo record per provare e estriamo i due elementi. Scriviamo una funzione che controlla se gli elementi di una copia sono nella riga"
      ],
      "id": "7UGC-sTeP-R7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm22mtPROAXc"
      },
      "source": [
        "coppia = ['nm0063086', 'nm0183823']    #primi due attori del primo record\n",
        "\n",
        "def row_contains_elements(row, elements):\n",
        "  return all(x in row for x in elements)\n",
        "\n",
        "data.map(lambda x:x[1]).filter(lambda x: row_contains_elements(x,coppia)).take(5)\n"
      ],
      "id": "jm22mtPROAXc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVckVsR_xDWP"
      },
      "source": [
        "Proviamo ora a cercare di far generare le copie possibili ad ogni singola riga. trick per evitare doppioni. flatmap direttamente almeno sono gia spacchettate"
      ],
      "id": "dVckVsR_xDWP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u96RReA1hh9"
      },
      "source": [
        "data.take(1)"
      ],
      "id": "8u96RReA1hh9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk-9ihyWoxh_"
      },
      "source": [
        "def generate_candidate(x):\n",
        "  candidates = []\n",
        "  for a, elemA in enumerate(x):\n",
        "    for b, elemB in enumerate(x):\n",
        "      if a < b:\n",
        "        candidates.append((elemA, elemB))\n",
        "  return candidates\n",
        "\n",
        "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)).take(10)"
      ],
      "id": "Gk-9ihyWoxh_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTp40IkixO4x"
      },
      "source": [
        "Aggiungiamo poi un controllo che la copia generata sia in quelle di interesse"
      ],
      "id": "PTp40IkixO4x"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w75-jNucwKwQ"
      },
      "source": [
        "copia = [('nm0063086', 'nm0183823'), ('nm0846894', 'nm3002376')]\n",
        "\n",
        "def generate_candidate(x):\n",
        "  candidates = []\n",
        "  for a, elemA in enumerate(x):\n",
        "    for b, elemB in enumerate(x):\n",
        "      if a < b:\n",
        "        candidates.append((elemA, elemB))\n",
        "  return candidates\n",
        "\n",
        "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)).filter(lambda x: x in copia).take(3)"
      ],
      "id": "w75-jNucwKwQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxuUOII3xV_w"
      },
      "source": [
        "Vero proprio passo di map. Le tuple per qualche motivo sono hashabili"
      ],
      "id": "yxuUOII3xV_w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK5xVYz1xU2W"
      },
      "source": [
        "copia = [('nm0063086', 'nm0183823'), ('nm0846894', 'nm3002376')]\n",
        "\n",
        "def generate_candidate(x):\n",
        "  candidates = []\n",
        "  for a, elemA in enumerate(x):\n",
        "    for b, elemB in enumerate(x):\n",
        "      if a < b:\n",
        "        candidates.append((elemA, elemB))\n",
        "  return candidates\n",
        "\n",
        "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)) \\\n",
        "    .filter(lambda x: x in copia).map(lambda x: (x,1)).take(3)"
      ],
      "id": "cK5xVYz1xU2W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ykb2KwxVGu"
      },
      "source": [
        "Aggiungiamo reduce e il controllo del threshold"
      ],
      "id": "N-ykb2KwxVGu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOWrjKjYxCAG"
      },
      "source": [
        "copia = [('nm0063086', 'nm0183823'), ('nm0846894', 'nm3002376')]\n",
        "\n",
        "def generate_candidate(x):\n",
        "  candidates = []\n",
        "  for a, elemA in enumerate(x):\n",
        "    for b, elemB in enumerate(x):\n",
        "      if a < b:\n",
        "        candidates.append((elemA, elemB))\n",
        "  return candidates\n",
        "\n",
        "data.map(lambda x: x[1]).flatMap(lambda x: generate_candidate(x)) \\\n",
        "    .filter(lambda x: x in copia) \\\n",
        "    .map(lambda x: (x,1)) \\\n",
        "    .reduceByKey(lambda a,b: a+b) \\\n",
        "    .filter(lambda x: x[1] >=1) \\\n",
        "    .take(3)\n",
        "          "
      ],
      "id": "nOWrjKjYxCAG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GsAUCCbg8ON"
      },
      "source": [
        ""
      ],
      "id": "_GsAUCCbg8ON",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7dZSxBd1hWQ"
      },
      "source": [
        "# Apriori with MAP-REDUCE"
      ],
      "id": "I7dZSxBd1hWQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYDq3kNh5aql"
      },
      "source": [
        "Follow an implementatio of the Apriori algorithm using a map-reduce approach"
      ],
      "id": "YYDq3kNh5aql"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z0pKUxbAgTF"
      },
      "source": [
        "def apriorihmap(data, support_threshold):\n",
        "    \"\"\" \n",
        "    data: Pyspark.rdd \n",
        "      [\n",
        "        [tconst, [nconst,]],\n",
        "      ]\n",
        "    \"\"\"\n",
        "    nconst_rdd = data.map(lambda x: x[1])\n",
        "\n",
        "    frequent_items_rdd = nconst_rdd.flatMap(lambda x: x) \\\n",
        "          .map(lambda elem: (elem,1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .filter(lambda x: x[1] >= support_threshold)\n",
        "\n",
        "    #print(f\"found {frequent_items_rdd.count()} frequent singletons\")\n",
        "    frequent_singletons_bv = spark.sparkContext.broadcast({k[0]:True for k in frequent_items_rdd.collect()})\n",
        "\n",
        "    def generate_candidate(x):\n",
        "      candidates = []\n",
        "      for a, elemA in enumerate(x):\n",
        "        for b, elemB in enumerate(x):\n",
        "          if a < b:\n",
        "            candidates.append((elemA, elemB))\n",
        "      return candidates\n",
        "    \n",
        "    frequent_couples_rdd = data.map(lambda x: x[1]) \\\n",
        "          .filter(lambda x: [elem for elem in x if frequent_singletons_bv.value.get(elem, False)])\\\n",
        "          .flatMap(lambda x: generate_candidate(x)) \\\n",
        "          .map(lambda x: (x,1)) \\\n",
        "          .reduceByKey(lambda a,b: a+b) \\\n",
        "          .filter(lambda x: x[1] >=support_threshold)\n",
        "\n",
        "    return frequent_items_rdd.union(frequent_couples_rdd)"
      ],
      "id": "1z0pKUxbAgTF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Ac0zO_BeYT"
      },
      "source": [
        "rules = apriorihmap(data, 60)\n",
        "rules.count()"
      ],
      "id": "c-Ac0zO_BeYT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tb7Li75hvFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2af0047-d459-4abb-f8d5-cab523d2499f"
      },
      "source": [
        "rules.take(5)"
      ],
      "id": "8tb7Li75hvFU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('nm0003425', 62),\n",
              " ('nm0282348', 83),\n",
              " ('nm0723801', 126),\n",
              " ('nm0417837', 72),\n",
              " ('nm0165134', 77)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti7KSSed4h34"
      },
      "source": [
        "# Apriori classic"
      ],
      "id": "ti7KSSed4h34"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AghTTfDdxcAu"
      },
      "source": [
        "from itertools import tee\n",
        "\n",
        "def apriori(partitionData, support_threshold):\n",
        "  singleton_counter = []\n",
        "  lookup_index_table = {}\n",
        "  reverse_lookup_index_table = {}\n",
        "  \n",
        "  d1, d2 = tee(partitionData, 2)\n",
        "\n",
        "  # count singletons\n",
        "  for bucket in d1:\n",
        "    for item in bucket[1]:\n",
        "      if item not in lookup_index_table:\n",
        "        # The newly discovered element is appended on the tail of the array counter\n",
        "        lookup_index_table[item] = len(singleton_counter)\n",
        "        reverse_lookup_index_table[len(singleton_counter)] = item\n",
        "        singleton_counter.append(0)\n",
        "      idx = lookup_index_table[item]\n",
        "      singleton_counter[idx] += 1\n",
        "\n",
        "  frequent_items_table = [index for index,count in enumerate(singleton_counter) if count >= support_threshold]\n",
        "  frequent_singleton = [(reverse_lookup_index_table[item], singleton_counter[item]) for item in frequent_items_table]\n",
        "  # count pairs\n",
        "  pair_counter = {}\n",
        "  \n",
        "  for bucket in d2:\n",
        "      frequent_items_of_bucket = [lookup_index_table[item] for item in bucket[1] \n",
        "                        if lookup_index_table[item] in frequent_items_table]\n",
        "      \n",
        "      for x in frequent_items_of_bucket:\n",
        "          for y in frequent_items_of_bucket:\n",
        "              if x<y:\n",
        "                  pair_counter[(x,y)] = pair_counter.get((x,y), 0) + 1\n",
        "\n",
        "  frequent_couples = [((reverse_lookup_index_table[couple[0]], reverse_lookup_index_table[couple[1]]), count) for couple ,count \n",
        "                      in pair_counter.items() if count >= support_threshold]\n",
        "  \n",
        "  return iter(frequent_singleton + frequent_couples)"
      ],
      "id": "AghTTfDdxcAu",
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fc_EBDgiys-"
      },
      "source": [
        "apriori_res = list(apriori(minid.collect(), 5))"
      ],
      "id": "4Fc_EBDgiys-",
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp8ezn4uoJJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014cec26-e1df-45b0-d234-37bde4056f39"
      },
      "source": [
        "apriori_res"
      ],
      "id": "kp8ezn4uoJJZ",
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('nm0098376', 6),\n",
              " ('nm0292407', 8),\n",
              " ('nm0926280', 12),\n",
              " ('nm0448682', 7),\n",
              " ('nm0817956', 5),\n",
              " ('nm0165691', 5),\n",
              " ('nm0024706', 8),\n",
              " ('nm0829171', 7),\n",
              " ('nm0330352', 5),\n",
              " ('nm0681933', 6),\n",
              " ('nm0060238', 6),\n",
              " ('nm0731623', 6),\n",
              " ('nm0696049', 5),\n",
              " ('nm0624735', 5),\n",
              " ('nm0676473', 6),\n",
              " ('nm0516974', 6),\n",
              " ('nm0163255', 5),\n",
              " ('nm0057585', 5),\n",
              " ('nm0728230', 5),\n",
              " ('nm0392059', 9),\n",
              " ('nm0292810', 5),\n",
              " ('nm0613115', 5),\n",
              " ('nm0068213', 5),\n",
              " ('nm0386893', 5),\n",
              " ('nm0148546', 5),\n",
              " ('nm0624714', 5),\n",
              " ('nm0314700', 5),\n",
              " (('nm0292407', 'nm0926280'), 5),\n",
              " (('nm0829171', 'nm0330352'), 5),\n",
              " (('nm0024706', 'nm0613115'), 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KsAA5w_21g1"
      },
      "source": [
        "# SON"
      ],
      "id": "0KsAA5w_21g1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJgYWyXy28lB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a4b4320-26bd-4569-d176-833029225bc0"
      },
      "source": [
        "# empirical sweet-spot for the number of partitions (assuming every executor has 4 cores ...)\n",
        "num_partitions = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().size() * 4\n",
        "num_partitions"
      ],
      "id": "vJgYWyXy28lB",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phfT-KHWk196"
      },
      "source": [
        "d = data.repartition(num_partitions)"
      ],
      "id": "phfT-KHWk196",
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZioc10sr5ta",
        "outputId": "92a93396-a1eb-4af8-c26d-76605bbffc82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "minid = d.take(500)\n",
        "minid = spark.sparkContext.parallelize(minid)\n",
        "minid = minid.repartition(num_partitions)\n",
        "minid.take(5)"
      ],
      "id": "GZioc10sr5ta",
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(tconst='tt0000886', nconsts=['nm0609814']),\n",
              " Row(tconst='tt0000891', nconsts=['nm0727622', 'nm0814379']),\n",
              " Row(tconst='tt0000941', nconsts=['nm0034453', 'nm0140054', 'nm0243918', 'nm0294022']),\n",
              " Row(tconst='tt0000947', nconsts=['nm0488932', 'nm0814379']),\n",
              " Row(tconst='tt0000992', nconsts=['nm0119164'])]"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irTUU9zy6rNP"
      },
      "source": [
        "from itertools import tee\n",
        "\n",
        "def apriori_son(partitionData, support_threshold):\n",
        "  singleton_counter = []\n",
        "  lookup_index_table = {}\n",
        "  reverse_lookup_index_table = {}\n",
        "  \n",
        "  d1, d2 = tee(partitionData, 2)\n",
        "\n",
        "  # count singletons\n",
        "  for bucket in d1:\n",
        "    for item in bucket[1]:\n",
        "      if item not in lookup_index_table:\n",
        "        # The newly discovered element is appended on the tail of the array counter\n",
        "        lookup_index_table[item] = len(singleton_counter)\n",
        "        reverse_lookup_index_table[len(singleton_counter)] = item\n",
        "        singleton_counter.append(0)\n",
        "      idx = lookup_index_table[item]\n",
        "      singleton_counter[idx] += 1\n",
        "\n",
        "  frequent_items_table = [index for index,count in enumerate(singleton_counter) if count >= support_threshold]\n",
        "  frequent_singleton = [(reverse_lookup_index_table[item], singleton_counter[item]) for item in frequent_items_table]\n",
        "  # count pairs\n",
        "  pair_counter = {}\n",
        "  \n",
        "  for bucket in d2:\n",
        "      frequent_items_of_bucket = [lookup_index_table[item] for item in bucket[1] \n",
        "                        if lookup_index_table[item] in frequent_items_table]\n",
        "      \n",
        "      for x in frequent_items_of_bucket:\n",
        "          for y in frequent_items_of_bucket:\n",
        "              if x<y:\n",
        "                  pair_counter[(x,y)] = pair_counter.get((x,y), 0) + 1\n",
        "\n",
        "  frequent_couples = [((reverse_lookup_index_table[couple[0]], reverse_lookup_index_table[couple[1]]), count) for couple ,count \n",
        "                      in pair_counter.items() if count >= support_threshold]\n",
        "  \n",
        "  return iter(frequent_singleton + frequent_couples)"
      ],
      "id": "irTUU9zy6rNP",
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6lKtG-gpYk"
      },
      "source": [
        "def count_in_partition(data, frequent):\n",
        "  # prepare data for processing\n",
        "  frequent = frequent.value   # extract broadcasted values\n",
        "  data = list(data)           # cast to list to iterate over all frequent items\n",
        "\n",
        "  # check foreach frequent itemset\n",
        "  for frequent_item in frequent:\n",
        "    # trick to cast single element to list  not remove duplicate using set()\n",
        "    if type(frequent_item) is not tuple:\n",
        "      to_check = [frequent_item]\n",
        "    else:\n",
        "      to_check = frequent_item\n",
        "      \n",
        "    c = 0     # counter\n",
        "    # and foreach row of the dataset\n",
        "    for itemset in data:\n",
        "      # check if the frequent itemset is subset of the items of the row\n",
        "      if set(to_check).issubset(itemset[1]):\n",
        "        c += 1\n",
        "    yield (frequent_item, c)"
      ],
      "id": "ff6lKtG-gpYk",
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImC5plrpZ4wv"
      },
      "source": [
        "def son_m_r(data, support):\n",
        "  reduced_support = support//data.getNumPartitions()\n",
        "  first_map = data.mapPartitions(lambda partition: apriori_son(partition, reduced_support)).map(lambda x: (x[0], 1))\n",
        "  first_reduce = first_map.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "  frequent_items = spark.sparkContext.broadcast([x[0] for x in first_reduce.collect()])\n",
        "\n",
        "  second_map = data.mapPartitions(lambda partition: count_in_partition(partition, frequent_items))\n",
        "  second_reduce = second_map.reduceByKey(lambda a,b: a+b).filter(lambda x: x[1] >= support)\n",
        "  return second_reduce"
      ],
      "id": "ImC5plrpZ4wv",
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_Yz7l86oywd"
      },
      "source": [
        "a = son_m_r(minid,5)"
      ],
      "id": "j_Yz7l86oywd",
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3UpnZZYbtvP",
        "outputId": "4bd232ea-7c00-42d7-8121-5140a0cb218b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a.collect()"
      ],
      "id": "n3UpnZZYbtvP",
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('nm0165691', 5),\n",
              " ('nm0024706', 8),\n",
              " ('nm0330352', 5),\n",
              " ('nm0068213', 5),\n",
              " ('nm0926280', 12),\n",
              " ('nm0448682', 7),\n",
              " ('nm0817956', 5),\n",
              " ('nm0292810', 5),\n",
              " (('nm0829171', 'nm0330352'), 5),\n",
              " ('nm0624714', 5),\n",
              " (('nm0613115', 'nm0024706'), 5),\n",
              " (('nm0926280', 'nm0292407'), 5),\n",
              " ('nm0098376', 6),\n",
              " ('nm0292407', 8),\n",
              " ('nm0681933', 6),\n",
              " ('nm0731623', 6),\n",
              " ('nm0696049', 5),\n",
              " ('nm0516974', 6),\n",
              " ('nm0392059', 9),\n",
              " ('nm0148546', 5),\n",
              " ('nm0314700', 5),\n",
              " ('nm0829171', 7),\n",
              " ('nm0060238', 6),\n",
              " ('nm0624735', 5),\n",
              " ('nm0676473', 6),\n",
              " ('nm0163255', 5),\n",
              " ('nm0057585', 5),\n",
              " ('nm0728230', 5),\n",
              " (('nm0292407', 'nm0926280'), 5),\n",
              " ('nm0613115', 5),\n",
              " ('nm0386893', 5),\n",
              " (('nm0024706', 'nm0613115'), 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCdsRebrgPKd",
        "outputId": "68e814de-f7f3-4632-ed0a-8d81726e81ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "a.sort(key=myFunc)"
      ],
      "id": "FCdsRebrgPKd",
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-236-13eb0458d6c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmyFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'tuple' and 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4e2Xdbf4eZy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af5ae55f-74d0-40ae-d666-f2d2f964bf60"
      },
      "source": [
        "# force actual partitioning. If you don't introduce a fake partition-key, the reduceByKey you do when counting elements will count the elements globally.\n",
        "\n",
        "support_threshold = 150\n",
        "\n",
        "import random\n",
        "data.map(lambda x: ((random.randint(1, num_partitions), x), 1))\\\n",
        "    .reduceByKey(lambda x,y: x+y)\\\n",
        "    .filter(lambda x: x[1] >= support_threshold / num_partitions)\\\n",
        "    .map(lambda x: (x[0][1], x[1]))\\\n",
        "    .reduceByKey(lambda x,y: x+y)\\\n",
        "    .filter(lambda x: x[1] >= support_threshold)\\\n",
        "    .first()\n",
        "\n",
        "\n",
        "    #TODO "
      ],
      "id": "F4e2Xdbf4eZy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-581a3e2ec0e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_partitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msupport_threshold\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_partitions\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msupport_threshold\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \"\"\"\n\u001b[0;32m-> 1586\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 209) (d882a9b7c9c5 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3NcLCryldUa"
      },
      "source": [
        "# Demo FP Growht"
      ],
      "id": "z3NcLCryldUa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz6FTwiSlgtB"
      },
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "fpGrowth = FPGrowth(itemsCol=\"nconsts\", minSupport=0.0001, minConfidence=0.0001)\n",
        "model = fpGrowth.fit(basket_data)"
      ],
      "id": "Fz6FTwiSlgtB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oM56a8Jmjnb"
      },
      "source": [
        "# Display frequent itemsets.\n",
        "model.freqItemsets.show()\n",
        "items = model.freqItemsets"
      ],
      "id": "3oM56a8Jmjnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka2nZvdMmjzL"
      },
      "source": [
        "# Display generated association rules.\n",
        "model.associationRules.show()\n",
        "rules = model.associationRules"
      ],
      "id": "Ka2nZvdMmjzL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnnm_GJMmj4h"
      },
      "source": [
        "# transform examines the input items against all the association rules and summarize the consequents as prediction\n",
        "model.transform(basket_data).show()\n",
        "transformed = model.transform(basket_data)"
      ],
      "id": "Hnnm_GJMmj4h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6IRRQKJ2rF7"
      },
      "source": [
        ""
      ],
      "id": "y6IRRQKJ2rF7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acn29Pgj2rY3"
      },
      "source": [
        "# Demo Antonio"
      ],
      "id": "Acn29Pgj2rY3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhiddpa3kiQR"
      },
      "source": [
        "import pandas as pd"
      ],
      "id": "yhiddpa3kiQR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDn6fZxXA6Fs"
      },
      "source": [
        "Lets try to load some data in a Pandas Dataframe"
      ],
      "id": "QDn6fZxXA6Fs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB6Fpgfa1El1"
      },
      "source": [
        "actors_cols = {\n",
        "    \"original\": [\n",
        "        \"nconst\",  # actor unique id\n",
        "        \"knownForTitles\"  # move he/she is in\n",
        "    ],\n",
        "    \"renamed\": [\"actorId\", \"titles\"]\n",
        "}\n",
        "\n",
        "actors_df = pd.read_csv(\n",
        "    \"name.basics.tsv.gz\",\n",
        "    compression=\"gzip\",\n",
        "    sep='\\t',\n",
        "    usecols=actors_cols[\"original\"]\n",
        ")\n",
        "\n",
        "# clean and pre-process\n",
        "actors_df.columns = actors_cols[\"renamed\"]\n",
        "actors_df.drop(actors_df[actors_df.titles == \"\\\\N\"].index, inplace=True)\n",
        "actors_df.titles = actors_df.titles.apply(lambda x: x.split(\",\"))"
      ],
      "id": "EB6Fpgfa1El1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqBeMA_JqHQS"
      },
      "source": [
        "actors_df"
      ],
      "id": "lqBeMA_JqHQS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ddwv9h4kiQR"
      },
      "source": [
        "def apriori(transactions, support_threshold):\n",
        "    singleton_counter = []\n",
        "    lookup_index_table = {}\n",
        "    reverse_lookup_index_table = {}\n",
        "\n",
        "    # count singletons\n",
        "    for bucket in transactions:\n",
        "        for elem in bucket:\n",
        "            if elem not in lookup_index_table:\n",
        "                # The newly discovered element is appended on the tail of the array counter\n",
        "                lookup_index_table[elem] = len(singleton_counter)\n",
        "                reverse_lookup_index_table[len(singleton_counter)] = elem\n",
        "                singleton_counter.append(0)\n",
        "\n",
        "            idx = lookup_index_table[elem]\n",
        "            singleton_counter[idx] += 1\n",
        "\n",
        "    frequent_items_table = [i for i,v in enumerate(singleton_counter) if v > support_threshold]\n",
        "\n",
        "    # count pairs\n",
        "    pair_counter = {}\n",
        "    for bucket in transactions:\n",
        "        frequent_items = [lookup_index_table[item] for item in bucket \n",
        "                          if lookup_index_table[item] in frequent_items_table]\n",
        "\n",
        "        for x in frequent_items:\n",
        "            for y in frequent_items:\n",
        "                if x<y:\n",
        "                    pair_counter[(x,y)] = pair_counter.get((x,y), 0) +1 \n",
        "\n",
        "    return [list(map(lambda x: reverse_lookup_index_table[x], i)) for i,c in pair_counter.items() \n",
        "            if c > support_threshold] "
      ],
      "id": "4Ddwv9h4kiQR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "x0Lw1rLXyo03",
        "outputId": "1aede69d-1e71-4f84-eed0-4a20649ef4df"
      },
      "source": [
        "apriori(data, 20)"
      ],
      "id": "x0Lw1rLXyo03",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-d6c63b5c0959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mapriori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-81-149db111dd95>\u001b[0m in \u001b[0;36mapriori\u001b[0;34m(transactions, support_threshold)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# count singletons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlookup_index_table\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'RDD' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIQAUaiDkiQS"
      },
      "source": [
        "# test\n",
        "rules = apriori(actors_df.titles, 300)\n",
        "\n",
        "movies_df = pd.read_csv(\"title.basics.tsv.gz\", compression='gzip', sep='\\t')\n",
        "from IPython.display import display\n",
        "\n",
        "for x,y in rules:\n",
        "    display(movies_df.loc[((movies_df.tconst == x) | (movies_df.tconst == y))])"
      ],
      "id": "CIQAUaiDkiQS",
      "execution_count": null,
      "outputs": []
    }
  ]
}